{"url": "https://towardsdatascience.com/how-to-set-the-number-of-trees-in-random-forest/", "title": "How to Set the Number of Trees in Random Forest", "authors": ["Thomas Martin Lange", "Christabelle Pabalan", "Peng Qian", "Roberta Rocca", "Seth Levine", "Luciano Abriata", "David Harar", "Gustav \u0160\u00edr", ".Wp-Block-Post-Author-Name Box-Sizing Border-Box"], "publication_date": "2025-05-16T20:37:12+00:00", "text": "Scientific publication T. M. Lange, M. G\u00fcltas, A. O. Schmitt & F. Heinrich (2025). optRF: Optimising random forest stability by determining the optimal number of trees. BMC bioinformatics, 26(1), 95. Follow this LINK to the original publication.\n\nForest \u2014 A Powerful Tool for Anyone Working With Data\n\nWhat is Random Forest?\n\nHave you ever wished you could make better decisions using data \u2014 like predicting the risk of diseases, crop yields, or spotting patterns in customer behavior? That\u2019s where machine learning comes in and one of the most accessible and powerful tools in this field is something called Random Forest.\n\nSo why is random forest so popular? For one, it\u2019s incredibly flexible. It works well with many types of data whether numbers, categories, or both. It\u2019s also widely used in many fields \u2014 from predicting patient outcomes in healthcare to detecting fraud in finance, from improving shopping experiences online to optimising agricultural practices.\n\nDespite the name, random forest has nothing to do with trees in a forest \u2014 but it does use something called Decision Trees to make smart predictions. You can think of a decision tree as a flowchart that guides a series of yes/no questions based on the data you give it. A random forest creates a whole bunch of these trees (hence the \u201cforest\u201d), each slightly different, and then combines their results to make one final decision. It\u2019s a bit like asking a group of experts for their opinion and then going with the majority vote.\n\nBut until recently, one question was unanswered: How many decision trees do I actually need? If each decision tree can lead to different results, averaging many trees would lead to better and more reliable results. But how many are enough? Luckily, the optRF package answers this question!\n\nSo let\u2019s have a look at how to optimise Random Forest for predictions and variable selection!\n\nMaking Predictions with Random Forests\n\nTo optimise and to use random forest for making predictions, we can use the open-source statistics programme R. Once we open R, we have to install the two R packages \u201cranger\u201d which allows to use random forests in R and \u201coptRF\u201d to optimise random forests. Both packages are open-source and available via the official R repository CRAN. In order to install and load these packages, the following lines of R code can be run:\n\n> install.packages(\u201cranger\u201d) > install.packages(\u201coptRF\u201d) > library(ranger) > library(optRF)\n\nNow that the packages are installed and loaded into the library, we can use the functions that these packages contain. Furthermore, we can also use the data set included in the optRF package which is free to use under the GPL license (just as the optRF package itself). This data set called SNPdata contains in the first column the yield of 250 wheat plants as well as 5000 genomic markers (so called single nucleotide polymorphisms or SNPs) that can contain either the value 0 or 2.\n\n> SNPdata[1:5,1:5] Yield SNP_0001 SNP_0002 SNP_0003 SNP_0004 ID_001 670.7588 0 0 0 0 ID_002 542.5611 0 2 0 0 ID_003 591.6631 2 2 0 2 ID_004 476.3727 0 0 0 0 ID_005 635.9814 2 2 0 2\n\nThis data set is an example for genomic data and can be used for genomic prediction which is a very important tool for breeding high-yielding crops and, thus, to fight world hunger. The idea is to predict the yield of crops using genomic markers. And exactly for this purpose, random forest can be used! That means that a random forest model is used to describe the relationship between the yield and the genomic markers. Afterwards, we can predict the yield of wheat plants where we only have genomic markers.\n\nTherefore, let\u2019s imagine that we have 200 wheat plants where we know the yield and the genomic markers. This is the so-called training data set. Let\u2019s further assume that we have 50 wheat plants where we know the genomic markers but not their yield. This is the so-called test data set. Thus, we separate the data frame SNPdata so that the first 200 rows are saved as training and the last 50 rows without their yield are saved as test data:\n\n> Training = SNPdata[1:200,] > Test = SNPdata[201:250,-1]\n\nWith these data sets, we can now have a look at how to make predictions using random forests!\n\nFirst, we got to calculate the optimal number of trees for random forest. Since we want to make predictions, we use the function opt_prediction from the optRF package. Into this function we have to insert the response from the training data set (in this case the yield), the predictors from the training data set (in this case the genomic markers), and the predictors from the test data set. Before we run this function, we can use the set.seed function to ensure reproducibility even though this is not necessary (we will see later why reproducibility is an issue here):\n\n> set.seed(123) > optRF_result = opt_prediction(y = Training[,1], + X = Training[,-1], + X_Test = Test) Recommended number of trees: 19000\n\nAll the results from the opt_prediction function are now saved in the object optRF_result, however, the most important information was already printed in the console: For this data set, we should use 19,000 trees.\n\nWith this information, we can now use random forest to make predictions. Therefore, we use the ranger function to derive a random forest model that describes the relationship between the genomic markers and the yield in the training data set. Also here, we have to insert the response in the y argument and the predictors in the x argument. Furthermore, we can set the write.forest argument to be TRUE and we can insert the optimal number of trees in the num.trees argument:\n\n> RF_model = ranger(y = Training[,1], x = Training[,-1], + write.forest = TRUE, num.trees = 19000)\n\nAnd that\u2019s it! The object RF_model contains the random forest model that describes the relationship between the genomic markers and the yield. With this model, we can now predict the yield for the 50 plants in the test data set where we have the genomic markers but we don\u2019t know the yield:\n\n> predictions = predict(RF_model, data=Test)$predictions > predicted_Test = data.frame(ID = row.names(Test), predicted_yield = predictions)\n\nThe data frame predicted_Test now contains the IDs of the wheat plants together with their predicted yield:\n\n> head(predicted_Test) ID predicted_yield ID_201 593.6063 ID_202 596.8615 ID_203 591.3695 ID_204 589.3909 ID_205 599.5155 ID_206 608.1031\n\nVariable Selection with Random Forests\n\nA different approach to analysing such a data set would be to find out which variables are most important to predict the response. In this case, the question would be which genomic markers are most important to predict the yield. Also this can be done with random forests!\n\nIf we tackle such a task, we don\u2019t need a training and a test data set. We can simply use the entire data set SNPdata and see which of the variables are the most important ones. But before we do that, we should again determine the optimal number of trees using the optRF package. Since we are insterested in calculating the variable importance, we use the function opt_importance :\n\n> set.seed(123) > optRF_result = opt_importance(y=SNPdata[,1], + X=SNPdata[,-1]) Recommended number of trees: 40000\n\nOne can see that the optimal number of trees is now higher than it was for predictions. This is actually often the case. However, with this number of trees, we can now use the ranger function to calculate the importance of the variables. Therefore, we use the ranger function as before but we change the number of trees in the num.trees argument to 40,000 and we set the importance argument to \u201cpermutation\u201d (other options are \u201cimpurity\u201d and \u201cimpurity_corrected\u201d).\n\n> set.seed(123) > RF_model = ranger(y=SNPdata[,1], x=SNPdata[,-1], + write.forest = TRUE, num.trees = 40000, + importance=\"permutation\") > D_VI = data.frame(variable = names(SNPdata)[-1], + importance = RF_model$variable.importance) > D_VI = D_VI[order(D_VI$importance, decreasing=TRUE),]\n\nThe data frame D_VI now contains all the variables, thus, all the genomic markers, and next to it, their importance. Also, we have directly ordered this data frame so that the most important markers are on the top and the least important markers are at the bottom of this data frame. Which means that we can have a look at the most important variables using the head function:\n\n> head(D_VI) variable importance SNP_0020 45.75302 SNP_0004 38.65594 SNP_0019 36.81254 SNP_0050 34.56292 SNP_0033 30.47347 SNP_0043 28.54312\n\nAnd that\u2019s it! We have used random forest to make predictions and to estimate the most important variables in a data set. Furthermore, we have optimised random forest using the optRF package!\n\nWhy Do We Need Optimisation?\n\nNow that we\u2019ve seen how easy it is to use random forest and how quickly it can be optimised, it\u2019s time to take a closer look at what\u2019s happening behind the scenes. Specifically, we\u2019ll explore how random forest works and why the results might change from one run to another.\n\nTo do this, we\u2019ll use random forest to calculate the importance of each genomic marker but instead of optimising the number of trees beforehand, we\u2019ll stick with the default settings in the ranger function. By default, ranger uses 500 decision trees. Let\u2019s try it out:\n\n> set.seed(123) > RF_model = ranger(y=SNPdata[,1], x=SNPdata[,-1], + write.forest = TRUE, importance=\"permutation\") > D_VI = data.frame(variable = names(SNPdata)[-1], + importance = RF_model$variable.importance) > D_VI = D_VI[order(D_VI$importance, decreasing=TRUE),] > head(D_VI) variable importance SNP_0020 80.22909 SNP_0019 60.37387 SNP_0043 50.52367 SNP_0005 43.47999 SNP_0034 38.52494 SNP_0015 34.88654\n\nAs expected, everything runs smoothly \u2014 and quickly! In fact, this run was significantly faster than when we previously used 40,000 trees. But what happens if we run the exact same code again but this time with a different seed?\n\n> set.seed(321) > RF_model2 = ranger(y=SNPdata[,1], x=SNPdata[,-1], + write.forest = TRUE, importance=\"permutation\") > D_VI2 = data.frame(variable = names(SNPdata)[-1], + importance = RF_model2$variable.importance) > D_VI2 = D_VI2[order(D_VI2$importance, decreasing=TRUE),] > head(D_VI2) variable importance SNP_0050 60.64051 SNP_0043 58.59175 SNP_0033 52.15701 SNP_0020 51.10561 SNP_0015 34.86162 SNP_0019 34.21317\n\nOnce again, everything appears to work fine but take a closer look at the results. In the first run, SNP_0020 had the highest importance score at 80.23, but in the second run, SNP_0050 takes the top spot and SNP_0020 drops to the fourth place with a much lower importance score of 51.11. That\u2019s a significant shift! So what changed?\n\nThe answer lies in something called non-determinism. Random forest, as the name suggests, involves a lot of randomness: it randomly selects data samples and subsets of variables at various points during training. This randomness helps prevent overfitting but it also means that results can vary slightly each time you run the algorithm \u2014 even with the exact same data set. That\u2019s where the set.seed() function comes in. It acts like a bookmark in a shuffled deck of cards. By setting the same seed, you ensure that the random choices made by the algorithm follow the same sequence every time you run the code. But when you change the seed, you\u2019re effectively changing the random path the algorithm follows. That\u2019s why, in our example, the most important genomic markers came out differently in each run. This behavior \u2014 where the same process can yield different results due to internal randomness \u2014 is a classic example of non-determinism in machine learning.\n\nTaming the Randomness in Random Forests\n\nAs we just saw, random forest models can produce slightly different results every time you run them even when using the same data due to the algorithm\u2019s built-in randomness. So, how can we reduce this randomness and make our results more stable?\n\nOne of the simplest and most effective ways is to increase the number of trees. Each tree in a random forest is trained on a random subset of the data and variables, so the more trees we add, the better the model can \u201caverage out\u201d the noise caused by individual trees. Think of it like asking 10 people for their opinion versus asking 1,000 \u2014 you\u2019re more likely to get a reliable answer from the larger group.\n\nWith more trees, the model\u2019s predictions and variable importance rankings tend to become more stable and reproducible even without setting a specific seed. In other words, adding more trees helps to tame the randomness. However, there\u2019s a catch. More trees also mean more computation time. Training a random forest with 500 trees might take a few seconds but training one with 40,000 trees could take several minutes or more, depending on the size of your data set and your computer\u2019s performance.\n\nHowever, the relationship between the stability and the computation time of random forest is non-linear. While going from 500 to 1,000 trees can significantly improve stability, going from 5,000 to 10,000 trees might only provide a tiny improvement in stability while doubling the computation time. At some point, you hit a plateau where adding more trees gives diminishing returns \u2014 you pay more in computation time but gain very little in stability. That\u2019s why it\u2019s essential to find the right balance: Enough trees to ensure stable results but not so many that your analysis becomes unnecessarily slow.\n\nAnd this is exactly what the optRF package does: it analyses the relationship between the stability and the number of trees in random forests and uses this relationship to determine the optimal number of trees that leads to stable results and beyond which adding more trees would unnecessarily increase the computation time.\n\nAbove, we have already used the opt_importance function and saved the results as optRF_result. This object contains the information about the optimal number of trees but it also contains information about the relationship between the stability and the number of trees. Using the plot_stability function, we can visualise this relationship. Therefore, we have to insert the name of the optRF object, which measure we are interested in (here, we are interested in the \u201cimportance\u201d), the interval we want to visualise on the X axis, and if the recommended number of trees should be added:\n\n> plot_stability(optRF_result, measure=\"importance\", + from=0, to=50000, add_recommendation=FALSE)\n\nThe output of the plot_stability function visualises the stability of random forest depending on the number of decision trees\n\nThis plot clearly shows the non-linear relationship between stability and the number of trees. With 500 trees, random forest only leads to a stability of around 0.2 which explains why the results changed drastically when repeating random forest after setting a different seed. With the recommended 40,000 trees, however, the stability is near 1 (which indicates a perfect stability). Adding more than 40,000 trees would get the stability further to 1 but this increase would be only very small while the computation time would further increase. That is why 40,000 trees indicate the optimal number of trees for this data set.\n\nThe Takeaway: Optimise Random Forest to Get the Most of It\n\nRandom forest is a powerful ally for anyone working with data \u2014 whether you\u2019re a researcher, analyst, student, or data scientist. It\u2019s easy to use, remarkably flexible, and highly effective across a wide range of applications. But like any tool, using it well means understanding what\u2019s happening under the hood. In this post, we\u2019ve uncovered one of its hidden quirks: The randomness that makes it strong can also make it unstable if not carefully managed. Fortunately, with the optRF package, we can strike the perfect balance between stability and performance, ensuring we get reliable results without wasting computational resources. Whether you\u2019re working in genomics, medicine, economics, agriculture, or any other data-rich field, mastering this balance will help you make smarter, more confident decisions based on your data."}