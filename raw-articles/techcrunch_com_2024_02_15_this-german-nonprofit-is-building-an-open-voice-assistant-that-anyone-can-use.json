{"url": "https://techcrunch.com/2024/02/15/this-german-nonprofit-is-building-an-open-voice-assistant-that-anyone-can-use/", "title": "This German nonprofit is building an open voice assistant that anyone can use", "authors": ["Kyle Wiggers", "Ai Editor", "Zack Whittaker", "Rebecca Bellan", "Amanda Silberling", "Tim De Chant", "Connie Loizos", "Maxwell Zeff", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media"], "publication_date": "2024-02-15T00:00:00", "text": "There have been many attempts at open source AI-powered voice assistants (see Rhasspy, Mycroft and Jasper, to name a few) \u2014 all established with the goal of creating privacy-preserving, offline experiences that don\u2019t compromise on functionality. But development\u2019s proven to be extraordinarily slow. That\u2019s because, in addition to all the usual challenges attendant with open source projects, programming an assistant is hard. Tech like Google Assistant, Siri and Alexa have years, if not decades, of R&D behind them \u2014 and enormous infrastructure to boot.\n\nBut that\u2019s not deterring the folks at Large-scale Artificial Intelligence Open Network (LAION), the German nonprofit responsible for maintaining some of the world\u2019s most popular AI training data sets. This month, LAION announced a new initiative, BUD-E, that seeks to build a \u201cfully open\u201d voice assistant capable of running on consumer hardware.\n\nWhy launch a whole new voice assistant project when there are countless others out there in various states of abandonment? Wieland Brendel, a fellow at the Ellis Institute and a contributor to BUD-E, believes there isn\u2019t an open assistant with an architecture extensible enough to take full advantage of emerging GenAI technologies, particularly large language models (LLMs) along the lines of OpenAI\u2019s ChatGPT.\n\n\u201cMost interactions with [assistants] rely on chat interfaces that are rather cumbersome to interact with, [and] the dialogues with those systems feel stilted and unnatural,\u201d Brendel told TechCrunch in an email interview. \u201cThose systems are OK to convey commands to control your music or turn on the light, but they\u2019re not a basis for long and engaging conversations. The goal of BUD-E is to provide the basis for a voice assistant that feels much more natural to humans and that mimics the natural speech patterns of human dialogues and remembers past conversations.\u201d\n\nBrendel added that LAION also wants to ensure that every component of BUD-E can eventually be integrated with apps and services license-free, even commercially \u2014 which isn\u2019t necessarily the case for other open assistant efforts.\n\nA collaboration with Ellis Institute in T\u00fcbingen, tech consultancy Collabora and the T\u00fcbingen AI Center, BUD-E \u2014 recursive shorthand for \u201cBuddy for Understanding and Digital Empathy\u201d \u2014 has an ambitious roadmap. In a blog post, the LAION team lays out what they hope to accomplish in the next few months, chiefly building \u201cemotional intelligence\u201d into BUD-E and ensuring it can handle conversations involving multiple speakers at once.\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\n\u201cThere\u2019s a big need for a well-working natural voice assistant,\u201d Brendel said. \u201cLAION has shown in the past that it\u2019s great at building communities, and the ELLIS Institute T\u00fcbingen and the T\u00fcbingen AI Center are committed to provide the resources to develop the assistant.\u201d\n\nBUD-E is up and running \u2014 you can download and install it today from GitHub on Ubuntu or Windows PC (macOS is coming) \u2014 but it\u2019s very clearly in the early stages.\n\nLAION patched together several open models to assemble an MVP, including Microsoft\u2019s Phi-2 LLM, Columbia\u2019s text-to-speech StyleTTS2 and Nvidia\u2019s FastConformer for speech-to-text. As such, the experience is a bit unoptimized. Getting BUD-E to respond to commands within about 500 milliseconds \u2014 in the range of commercial voice assistants such as Google Assistant and Alexa \u2014 requires a beefy GPU like Nvidia\u2019s RTX 4090.\n\nCollabora is working pro bono to adapt its open source speech recognition and text-to-speech models, WhisperLive and WhisperSpeech, for BUD-E.\n\n\u201cBuilding the text-to-speech and speech recognition solutions ourselves means we can customize them to a degree that isn\u2019t possible with closed models exposed through APIs,\u201d Jakub Piotr C\u0142apa, an AI researcher at Collabora and BUD-E team member, said in an email. \u201cCollabora initially started working on [open assistants] partially because we struggled to find a good text-to-speech solution for an LLM-based voice agent for one of our customers. We decided to join forces with the wider open source community to make our models more widely accessible and useful.\u201d\n\nIn the near term, LAION says it\u2019ll work to make BUD-E\u2019s hardware requirements less onerous and reduce the assistant\u2019s latency. A longer-horizon undertaking is building a dataset of dialogs to fine-tune BUD-E \u2014 as well as a memory mechanism to allow BUD-E to store information from previous conversations and a speech processing pipeline that can keep track of several people talking at once.\n\nI asked the team whether accessibility was a priority, considering speech recognition systems historically haven\u2019t performed well with languages that aren\u2019t English and accents that aren\u2019t Transatlantic. One Stanford study found that speech recognition systems from Amazon, IBM, Google, Microsoft and Apple were almost twice as likely to mishear Black speakers versus white speakers of the same age and gender.\n\nBrendel said that LAION\u2019s not ignoring accessibility \u2014 but that it\u2019s not an \u201cimmediate focus\u201d for BUD-E.\n\n\u201cThe first focus is on really redefining the experience of how we interact with voice assistants before generalizing that experience to more diverse accents and languages,\u201d Brendel said.\n\nTo that end, LAION has some pretty out-there ideas for BUD-E, ranging from an animated avatar to personifying the assistant to support for analyzing users\u2019 faces through webcams to account for their emotional state.\n\nThe ethics of that last bit \u2014 facial analysis \u2014 are a bit dicey, needless to say. But Robert Kaczmarczyk, a LAION co-founder, stressed that LAION will remain committed to safety.\n\n\u201c[We] adhere strictly to the safety and ethical guidelines formulated by the EU AI Act,\u201d he told TechCrunch via email \u2014 referring to the legal framework governing the sale and use of AI in the EU. The EU AI Act allows European Union member countries to adopt more restrictive rules and safeguards for \u201chigh-risk\u201d AI, including emotion classifiers.\n\n\u201cThis commitment to transparency not only facilitates the early identification and correction of potential biases, but also aids the cause of scientific integrity,\u201d Kaczmarczyk added. \u201cBy making our data sets accessible, we enable the broader scientific community to engage in research that upholds the highest standards of reproducibility.\u201d\n\nLAION\u2019s previous work hasn\u2019t been pristine in the ethical sense, and it\u2019s pursuing a somewhat controversial separate project at the moment on emotion detection. But perhaps BUD-E will be different; we\u2019ll have to wait and see."}