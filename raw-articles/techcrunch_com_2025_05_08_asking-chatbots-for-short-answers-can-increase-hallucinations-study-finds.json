{"url": "https://techcrunch.com/2025/05/08/asking-chatbots-for-short-answers-can-increase-hallucinations-study-finds/", "title": "Asking chatbots for short answers can increase hallucinations, study finds", "authors": ["Kyle Wiggers", "Ai Editor", "Zack Whittaker", "Rebecca Bellan", "Amanda Silberling", "Tim De Chant", "Connie Loizos", "Maxwell Zeff", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media"], "publication_date": "2025-05-08T00:00:00", "text": "Turns out, telling an AI chatbot to be concise could make it hallucinate more than it otherwise would have.\n\nThat\u2019s according to a new study from Giskard, a Paris-based AI testing company developing a holistic benchmark for AI models. In a blog post detailing their findings, researchers at Giskard say prompts for shorter answers to questions, particularly questions about ambiguous topics, can negatively affect an AI model\u2019s factuality.\n\n\u201cOur data shows that simple changes to system instructions dramatically influence a model\u2019s tendency to hallucinate,\u201d wrote the researchers. \u201cThis finding has important implications for deployment, as many applications prioritize concise outputs to reduce [data] usage, improve latency, and minimize costs.\u201d\n\nHallucinations are an intractable problem in AI. Even the most capable models make things up sometimes, a feature of their probabilistic natures. In fact, newer reasoning models like OpenAI\u2019s o3 hallucinate more than previous models, making their outputs difficult to trust.\n\nIn its study, Giskard identified certain prompts that can worsen hallucinations, such as vague and misinformed questions asking for short answers (e.g. \u201cBriefly tell me why Japan won WWII\u201d). Leading models, including OpenAI\u2019s GPT-4o (the default model powering ChatGPT), Mistral Large, and Anthropic\u2019s Claude 3.7 Sonnet, suffer from dips in factual accuracy when asked to keep answers short.\n\nImage Credits:Giskard\n\nWhy? Giskard speculates that when told not to answer in great detail, models simply don\u2019t have the \u201cspace\u201d to acknowledge false premises and point out mistakes. Strong rebuttals require longer explanations, in other words.\n\n\u201cWhen forced to keep it short, models consistently choose brevity over accuracy,\u201d the researchers wrote. \u201cPerhaps most importantly for developers, seemingly innocent system prompts like \u2018be concise\u2019 can sabotage a model\u2019s ability to debunk misinformation.\u201d\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\nGiskard\u2019s study contains other curious revelations, like that models are less likely to debunk controversial claims when users present them confidently, and that models that users say they prefer aren\u2019t always the most truthful. Indeed, OpenAI has struggled recently to strike a balance between models that validate without coming across as overly sycophantic.\n\n\u201cOptimization for user experience can sometimes come at the expense of factual accuracy,\u201d wrote the researchers. \u201cThis creates a tension between accuracy and alignment with user expectations, particularly when those expectations include false premises.\u201d"}