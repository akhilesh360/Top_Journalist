{"url": "https://towardsdatascience.com/how-to-build-a-benchmark-for-your-models/", "title": "How To Build a Benchmark for Your Models", "authors": ["Lorenzo Mezzini", "Shreya Rao", "Piero Paialunga", "Luigi Battistoni", "Tds Editors", "Vadim Arzamasov", ".Wp-Block-Post-Author-Name Box-Sizing Border-Box"], "publication_date": "2025-05-15T20:15:00+00:00", "text": "I\u2019ve science consultant for the past three years, and I\u2019ve had the opportunity to work on multiple projects across various industries. Yet, I noticed one common denominator among most of the clients I worked with:\n\nThey rarely have a clear idea of the project objective.\n\nThis is one of the main obstacles data scientists face, especially now that Gen AI is taking over every domain.\n\nBut let\u2019s suppose that after some back and forth, the objective becomes clear. We managed to pin down a specific question to answer. For example:\n\nI want to classify my customers into two groups according to their probability to churn: \u201chigh likelihood to churn\u201d and \u201clow likelihood to churn\u201d\n\nWell, now what? Easy, let\u2019s start building some models!\n\nWrong!\n\nIf having a clear objective is rare, having a reliable benchmark is even rarer.\n\nIn my opinion, one of the most important steps in delivering a data science project is defining and agreeing on a set of benchmarks with the client.\n\nIn this blog post, I\u2019ll explain:\n\nWhat a benchmark is,\n\nWhy it is important to have a benchmark,\n\nHow I would build one using an example scenario and\n\nSome potential drawbacks to keep in mind\n\nWhat is a benchmark?\n\nA benchmark is a standardized way to evaluate the performance of a model. It provides a reference point against which new models can be compared.\n\nA benchmark needs two key components to be considered complete:\n\nA set of metrics to evaluate the performance A set of simple models to use as baselines\n\nThe concept at its core is simple: every time I develop a new model I compare it against both previous versions and the baseline models. This ensures improvements are real and tracked.\n\nIt is essential to understand that this baseline shouldn\u2019t be model or dataset-specific, but rather business-case-specific. It should be a general benchmark for a given business case.\n\nIf I encounter a new dataset, with the same business objective, this benchmark should be a reliable reference point.\n\nWhy building a benchmark is important\n\nNow that we\u2019ve defined what a benchmark is, let\u2019s dive into why I believe it\u2019s worth spending an extra project week on the development of a strong benchmark.\n\nWithout a Benchmark you\u2019re aiming for perfection \u2014 If you are working without a clear reference point any result will lose meaning. \u201cMy model has a MAE of 30.000\u201d Is that good? IDK! Maybe with a simple mean you would get a MAE of 25.000. By comparing your model to a baseline, you can measure both performance and improvement. Improves Communicating with Clients \u2014 Clients and business teams might not immediately understand the standard output of a model. However, by engaging them with simple baselines from the start, it becomes easier to demonstrate improvements later. In many cases benchmarks could come directly from the business in different shapes or forms. Helps in Model Selection \u2014 A benchmark gives a starting point to compare multiple models fairly. Without it, you might waste time testing models that aren\u2019t worth considering. Model Drift Detection and Monitoring \u2014 Models can degrade over time. By having a benchmark you might be able to intercept drifts early by comparing new model outputs against past benchmarks and baselines. Consistency Between Different Datasets \u2014 Datasets evolve. By having a fixed set of metrics and models you ensure that performance comparisons remain valid over time.\n\nWith a clear benchmark, every step in the model development will provide immediate feedback, making the whole process more intentional and data-driven.\n\nHow I would build a benchmark\n\nI hope I\u2019ve convinced you of the importance of having a benchmark. Now, let\u2019s actually build one.\n\nLet\u2019s start from the business question we presented at the very beginning of this blog post:\n\nI want to classify my customers into two groups according to their probability to churn: \u201chigh likelihood to churn\u201d and \u201clow likelihood to churn\u201d\n\nFor simplicity, I\u2019ll assume no additional business constraints, but in real-world scenarios, constraints often exist.\n\nFor this example, I am using this dataset (CC0: Public Domain). The data contains some attributes from a company\u2019s customer base (e.g., age, sex, number of products, \u2026) along with their churn status.\n\nNow that we have something to work on let\u2019s build the benchmark:\n\n1. Defining the metrics\n\nWe are dealing with a churn use case, in particular, this is a binary classification problem. Thus the main metrics that we could use are:\n\nPrecision \u2014 Percentage of correctly predicted churners among all predicted churners\n\nPercentage of correctly predicted churners among all predicted churners Recall \u2014 Percentage of actual churners correctly identified\n\nPercentage of actual churners correctly identified F1 score \u2014 Balances precision and recall\n\nBalances precision and recall True Positives, False Positives, True Negative and False Negatives\n\nThese are some of the \u201csimple\u201d metrics that could be used to evaluate the output of a model.\n\nHowever, it is not an exhaustive list, standard metrics aren\u2019t always enough. In many use cases, it might be useful to build custom metrics.\n\nLet\u2019s assume that in our business case the customers labeled as \u201chigh likelihood to churn\u201d are offered a discount. This creates:\n\nA cost ($250) when offering the discount to a non-churning customer\n\n($250) when offering the discount to a non-churning customer A profit ($1000) when retaining a churning customer\n\nFollowing on this definition we can build a custom metric that will be crucial in our scenario:\n\n# Defining the business case-specific reference metric def financial_gain(y_true, y_pred): loss_from_fp = np.sum(np.logical_and(y_pred == 1, y_true == 0)) * 250 gain_from_tp = np.sum(np.logical_and(y_pred == 1, y_true == 1)) * 1000 return gain_from_tp - loss_from_fp\n\nWhen you are building business-driven metrics these are usually the most relevant. Such metrics could take any shape or form: Financial goals, minimum requirements, percentage of coverage and more.\n\n2. Defining the benchmarks\n\nNow that we\u2019ve defined our metrics, we can define a set of baseline models to be used as a reference.\n\nIn this phase, you should define a list of simple-to-implement model in their simplest possible setup. There is no reason at this state to spend time and resources on the optimization of these models, my mindset is:\n\nIf I had 15 minutes, how would I implement this model?\n\nIn later phases of the model, you can add mode baseline models as the project proceeds.\n\nIn this case, I will use the following models:\n\nRandom Model \u2014 Assigns labels randomly\n\nAssigns labels randomly Majority Model \u2014 Always predicts the most frequent class\n\nAlways predicts the most frequent class Simple XGB\n\nSimple KNN\n\nimport numpy as np import xgboost as xgb from sklearn.neighbors import KNeighborsClassifier class BinaryMean(): @staticmethod def run_benchmark(df_train, df_test): np.random.seed(21) return np.random.choice(a=[1, 0], size=len(df_test), p=[df_train['y'].mean(), 1 - df_train['y'].mean()]) class SimpleXbg(): @staticmethod def run_benchmark(df_train, df_test): model = xgb.XGBClassifier() model.fit(df_train.select_dtypes(include=np.number).drop(columns='y'), df_train['y']) return model.predict(df_test.select_dtypes(include=np.number).drop(columns='y')) class MajorityClass(): @staticmethod def run_benchmark(df_train, df_test): majority_class = df_train['y'].mode()[0] return np.full(len(df_test), majority_class) class SimpleKNN(): @staticmethod def run_benchmark(df_train, df_test): model = KNeighborsClassifier() model.fit(df_train.select_dtypes(include=np.number).drop(columns='y'), df_train['y']) return model.predict(df_test.select_dtypes(include=np.number).drop(columns='y'))\n\nAgain, as in the case of the metrics, we can build custom benchmarks.\n\nLet\u2019s assume that in our business case the the marketing team contacts every client who\u2019s:\n\nOver 50 y/o and\n\nand That is not active anymore\n\nFollowing this rule we can build this model:\n\n# Defining the business case-specific benchmark class BusinessBenchmark(): @staticmethod def run_benchmark(df_train, df_test): df = df_test.copy() df.loc[:,'y_hat'] = 0 df.loc[(df['IsActiveMember'] == 0) & (df['Age'] >= 50), 'y_hat'] = 1 return df['y_hat']\n\nRunning the benchmark\n\nTo run the benchmark I will use the following class. The entry point is the method compare_with_benchmark() that, given a prediction, runs all the models and calculates all the metrics.\n\nimport numpy as np class ChurnBinaryBenchmark(): def __init__( self, metrics = [], benchmark_models = [], ): self.metrics = metrics self.benchmark_models = benchmark_models def compare_pred_with_benchmark( self, df_train, df_test, my_predictions, ): output_metrics = { 'Prediction': self._calculate_metrics(df_test['y'], my_predictions) } dct_benchmarks = {} for model in self.benchmark_models: dct_benchmarks[model.__name__] = model.run_benchmark(df_train = df_train, df_test = df_test) output_metrics[f'Benchmark - {model.__name__}'] = self._calculate_metrics(df_test['y'], dct_benchmarks[model.__name__]) return output_metrics def _calculate_metrics(self, y_true, y_pred): return {getattr(func, '__name__', 'Unknown') : func(y_true = y_true, y_pred = y_pred) for func in self.metrics}\n\nNow all we need is a prediction. For this example, I made a quick feature engineering and some hyperparameter tuning.\n\nThe last step is just to run the benchmark:\n\nbinary_benchmark = ChurnBinaryBenchmark( metrics=[f1_score, precision_score, recall_score, tp, tn, fp, fn, financial_gain], benchmark_models=[BinaryMean, SimpleXbg, MajorityClass, SimpleKNN, BusinessBenchmark] ) res = binary_benchmark.compare_pred_with_benchmark( df_train=df_train, df_test=df_test, my_predictions=preds, ) pd.DataFrame(res)\n\nBenchmark metrics comparison | Image by Author\n\nThis generates a comparison table of all models across all metrics. Using this table, it is possible to draw concrete conclusions on the model\u2019s predictions and make informed decisions on the following steps of the process.\n\nSome drawbacks\n\nAs we\u2019ve seen there are plenty of reasons why it is useful to have a benchmark. However, even though benchmarks are incredibly useful, there are some pitfalls to watch out for:\n\nNon-Informative Benchmark \u2014 When the metrics or models are poorly defined the marginal impact of having a benchmark decreases. Always define meaningful baselines. Misinterpretation by Stakeholders \u2014 Communication with the client is essential, it is important to state clearly what the metrics are measuring. The best model might not be the best on all the defined metrics. Overfitting to the Benchmark \u2014 You might end up trying to create features that are too specific, that might beat the benchmark, but do not generalize well in prediction. Don\u2019t focus on beating the benchmark, but on creating the best solution possible to the problem. Change of Objective \u2014 Objectives defined might change, due to miscommunication or changes in plans. Keep your benchmark flexible so it can adapt when needed.\n\nFinal thoughts\n\nBenchmarks provide clarity, ensure improvements are measurable, and create a shared reference point between data scientists and clients. They help avoid the trap of assuming a model is performing well without proof and ensure that every iteration brings real value.\n\nThey also act as a communication tool, making it easier to explain progress to clients. Instead of just presenting numbers, you can show clear comparisons that highlight improvements.\n\nHere you can find a notebook with a full implementation from this blog post."}