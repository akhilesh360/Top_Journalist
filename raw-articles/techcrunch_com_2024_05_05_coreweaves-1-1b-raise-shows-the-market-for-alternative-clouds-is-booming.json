{"url": "https://techcrunch.com/2024/05/05/coreweaves-1-1b-raise-shows-the-market-for-alternative-clouds-is-booming/", "title": "CoreWeave\u2019s $1.1B raise shows the market for alternative clouds is booming", "authors": ["Kyle Wiggers", "Ai Editor", "Sean O'Kane", "Kirsten Korosec", "Tim De Chant", "Cindy Zackney", "Amanda Silberling", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media", "Min-Width"], "publication_date": "2024-05-05T00:00:00", "text": "The appetite for alternative clouds has never been bigger.\n\nCase in point: CoreWeave, the GPU infrastructure provider that began life as a cryptocurrency mining operation, this week raised $1.1 billion in new funding from investors including Coatue, Fidelity and Altimeter Capital. Reportedly valuing the startup at $19 billion post-money, the new financing brings CoreWeave\u2019s total raised to $5 billion in debt and equity \u2013 a remarkable figure for a less-than-a-decade-old company.\n\nIt\u2019s not just CoreWeave.\n\nLambda Labs, which also offers an array of cloud-hosted GPU instances, in early April secured a \u201cspecial purpose financing vehicle\u201d of up to $500 million months after closing a $320 million Series C round. The nonprofit Voltage Park, backed by crypto billionaire Jed McCaleb, last October announced that it\u2019s investing $500 million in GPU-backed data centers. And Together AI, a cloud GPU host that also conducts generative AI research, in March landed $106 million in a Salesforce-led round.\n\nSo why all the enthusiasm for \u2014 and cash pouring into \u2014 the alternative cloud space? In three words, generative artificial intelligence.\n\nAs the generative AI boom times continue, so does the demand for the hardware to run and train generative AI models at scale. GPUs, architecturally, are the logical choice for training, fine-tuning and running models because they contain thousands of cores that can work in parallel to perform the linear algebra equations that make up generative models.\n\nBut installing GPUs is expensive. So the cloud is what most devs and organizations opt to adopt instead.\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\nIncumbents in the cloud computing space \u2014 Amazon Web Services (AWS), Google Cloud and Microsoft Azure \u2014 offer no shortage of GPU and specialty hardware instances optimized for generative AI workloads. But for at least some models and projects, alternative clouds can end up being cheaper \u2014 and delivering better availability.\n\nOn CoreWeave, renting an Nvidia A100 40GB \u2014 one popular choice for model training and inferencing \u2014 costs $2.39 per hour, which works out to $1,200 per month. On Azure, the same GPU costs $3.40 per hour, or $2,482 per month; on Google Cloud, it\u2019s $3.67 per hour, or $2,682 per month.\n\nGiven generative AI workloads are usually performed on clusters of GPUs, the cost deltas quickly grow.\n\n\u201cCompanies like CoreWeave participate in a market we call specialty \u2018GPU as a service\u2019 cloud providers,\u201d Sid Nag, VP of cloud services and technologies at Gartner, told TechCrunch. \u201cGiven the high demand for GPUs, they offers an alternate to the hyperscalers, where they\u2019ve taken Nvidia GPUs and provided another route to market and access to those GPUs.\u201d\n\nNag points out that even some big tech firms have begun to lean on alternative cloud providers as they run up against compute capacity challenges.\n\nLast June, CNBC reported that Microsoft had signed a multi-billion-dollar deal with CoreWeave to ensure that OpenAI, the maker of ChatGPT and a close Microsoft partner, would have adequate compute power to train its generative AI models. Nvidia, the furnisher of the bulk of CoreWeave\u2019s chips, sees this as a desirable trend, perhaps for leverage reasons; it\u2019s said to have given some alternative cloud providers preferential access to its GPUs.\n\nLee Sustar, principal analyst at Forrester, sees cloud vendors like CoreWeave succeeding in part because they don\u2019t have the infrastructure \u201cbaggage\u201d that incumbent providers have to deal with.\n\n\u201cGiven hyperscaler dominance of the overall public cloud market, which demands vast investments in infrastructure and range of services that make little or no revenue, challengers like CoreWeave have an opportunity to succeed with a focus on premium AI services without the burden of hypercaler-level investments overall,\u201d he said.\n\nBut is this growth sustainable?\n\nSustar has his doubts. He believes that alternative cloud providers\u2019 expansion will be conditioned both by (1) whether they can continue to bring GPUs online in high volume and (2) offer them at competitively low prices.\n\nCompeting on pricing might become challenging down the line as incumbents like Google, Microsoft and AWS ramp up investments in custom hardware to run and train models. Google offers its TPUs; Microsoft recently unveiled two custom chips, Azure Maia and Azure Cobalt; and AWS has Trainium, Inferentia and Graviton.\n\n\u201cHypercalers will leverage their custom silicon to mitigate their dependencies on Nvidia, while Nvidia will look to CoreWeave and other GPU-centric AI clouds,\u201d Sustar said.\n\nThen there\u2019s the fact that, while many generative AI workloads run best on GPUs, not all workloads need them \u2014 particularly if they\u2019re aren\u2019t time-sensitive. CPUs can run the necessary calculations, but typically slower than GPUs and custom hardware.\n\nIn the realm of existential concerns for alternative cloud providers is the threat that the generative AI bubble bursts, which would leave providers with mounds of GPUs and not nearly enough customers demanding them. But the future looks rosy in the short term, say Sustar and Nag, both of whom are expecting a steady stream of upstart clouds.\n\n\u201cGPU-oriented cloud startups will give [incumbents] plenty of competition, especially among customers who are already multi-cloud and can handle the complexity of management, security, risk and compliance across multiple clouds,\u201d Sustar said. \u201cThose sorts of cloud customers are comfortable trying out a new AI cloud if it has credible leadership, solid financial backing and GPUs with no wait times.\u201d"}