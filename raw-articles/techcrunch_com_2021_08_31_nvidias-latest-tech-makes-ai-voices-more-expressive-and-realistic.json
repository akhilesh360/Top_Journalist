{"url": "https://techcrunch.com/2021/08/31/nvidias-latest-tech-makes-ai-voices-more-expressive-and-realistic/", "title": "NVIDIA\u2019s latest tech makes AI voices more expressive and realistic", "authors": ["Steve Dent", "Zack Whittaker", "Rebecca Bellan", "Amanda Silberling", "Tim De Chant", "Connie Loizos", "Maxwell Zeff", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media", "Min-Width"], "publication_date": "2021-08-31T00:00:00", "text": "The voices on Amazon\u2019s Alexa, Google Assistant and other AI assistants are far ahead of old-school GPS devices, but they still lack the rhythms, intonation and other qualities that make speech sound, well, human. NVIDIA has unveiled new research and tools that can capture those natural speech qualities by letting you train the AI system with your own voice, the company announced at the Interspeech 2021 conference.\n\nTo improve its AI voice synthesis, NVIDIA\u2019s text-to-speech research team developed a model called RAD-TTS, a winning entry at an NAB broadcast convention competition to develop the most realistic avatar. The system allows an individual to train a text-to-speech model with their own voice, including the pacing, tonality, timbre and more.\n\nAnother RAD-TTS feature is voice conversion, which lets a user deliver one speaker\u2019s words using another person\u2019s voice. That interface gives fine, frame-level control over a synthesized voice\u2019s pitch, duration and energy.\n\nUsing this technology, NVIDIA\u2019s researchers created more conversational-sounding voice narration for its own I Am AI video series using synthesized rather than human voices. The aim was to get the narration to match the tone and style of the videos, something that hasn\u2019t been done well in many AI narrated videos to date. The results are still a bit robotic, but better than any AI narration I\u2019ve ever heard.\n\n\u201cWith this interface, our video producer could record himself reading the video script, and then use the AI model to convert his speech into the female narrator\u2019s voice. Using this baseline narration, the producer could then direct the AI like a voice actor \u2014 tweaking the synthesized speech to emphasize specific words, and modifying the pacing of the narration to better express the video\u2019s tone,\u201d NVIDIA wrote.\n\nNVIDIA is distributing some of this research \u2014 optimized to run efficiently on NVIDIA GPUs, of course \u2014 to anyone who wants to try it via open source through the NVIDIA NeMo Python toolkit for GPU-accelerated conversational AI, available on the company\u2019s NGC hub of containers and other software.\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\n\u201cSeveral of the models are trained with tens of thousands of hours of audio data on NVIDIA DGX systems. Developers can fine tune any model for their use cases, speeding up training using mixed-precision computing on NVIDIA Tensor Core GPUs,\u201d the company wrote.\n\nEditor\u2019s note: This post originally appeared on Engadget."}