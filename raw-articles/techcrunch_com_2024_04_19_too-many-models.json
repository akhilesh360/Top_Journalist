{"url": "https://techcrunch.com/2024/04/19/too-many-models/", "title": "Too many models", "authors": ["Devin Coldewey", "Writer", "Kyle Wiggers", "Amanda Silberling", "Rebecca Bellan", "Lorenzo Franceschi-Bicchierai", "Sean O'Kane", "Kirsten Korosec", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media"], "publication_date": "2024-04-19T00:00:00", "text": "How many AI models is too many? It depends on how you look at it, but 10 a week is probably a bit much. That\u2019s roughly how many we\u2019ve seen roll out in the last few days, and it\u2019s increasingly hard to say whether and how these models compare to one another, if it was ever possible to begin with. So what\u2019s the point?\n\nWe\u2019re at a weird time in the evolution of AI, though of course it\u2019s been pretty weird the whole time. We\u2019re seeing a proliferation of models large and small, from niche developers to large, well-funded ones.\n\nLet\u2019s just run down the list from this week, shall we? I\u2019ve tried to condense what sets each model apart.\n\nThat\u2019s 11, because one was announced while I was writing this. And this is not all of the models released or previewed this week! It\u2019s just the ones we saw and discussed. If we were to relax the conditions for inclusion a bit, there would dozens: some fine-tuned existing models, some combos like Idefics 2, some experimental or niche, and so on. Not to mention this week\u2019s new tools for building (torchtune) and battling against (Glaze 2.0) generative AI!\n\nWhat are we to make of this never-ending avalanche? We can\u2019t \u201creview\u201d them all. So how can we help you, our readers, understand and keep up with all these things?\n\nThe truth is you don\u2019t need to keep up. Some models like ChatGPT and Gemini have evolved into entire web platforms, spanning multiple use cases and access points. Other large language models like LLaMa or OLMo \u2014 though they technically share a basic architecture \u2014 don\u2019t actually fill the same role. They are intended to live in the background as a service or component, not in the foreground as a name brand.\n\nThere\u2019s some deliberate confusion about these two things, because the models\u2019 developers want to borrow a little of the fanfare associated with major AI platform releases, like your GPT-4V or Gemini Ultra. Everyone wants you to think that their release is an important one. And while it\u2019s probably important to somebody, that somebody is almost certainly not you.\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\nThink about it in the sense of another broad, diverse category like cars. When they were first invented, you just bought \u201ca car.\u201d Then a little later, you could choose between a big car, a small car, and a tractor. Nowadays, there are hundreds of cars released every year, but you probably don\u2019t need to be aware of even one in ten of them, because nine out of ten are not a car you need or even a car as you understand the term. Similarly, we\u2019re moving from the big/small/tractor era of AI toward the proliferation era, and even AI specialists can\u2019t keep up with and test all the models coming out.\n\nThe other side of this story is that we were already in this stage long before ChatGPT and the other big models came out. Far fewer people were reading about this 7 or 8 years ago, but we covered it nevertheless because it was clearly a technology waiting for its breakout moment. There were papers, models, and research constantly coming out, and conferences like SIGGRAPH and NeurIPS were filled with machine learning engineers comparing notes and building on one another\u2019s work. Here\u2019s a visual understanding story I wrote in 2011!\n\nThat activity is still underway every day. But because AI has become big business \u2014 arguably the biggest in tech right now \u2014 these developments have been lent a bit of extra weight, since people are curious whether one of these might be as big a leap over ChatGPT that ChatGPT was over its predecessors.\n\nThe simple truth is that none of these models is going to be that kind of big step, since OpenAI\u2019s advance was built on a fundamental change to machine learning architecture that every other company has now adopted, and which has not been superseded. Incremental improvements like a point or two better on a synthetic benchmark, or marginally more convincing language or imagery, is all we have to look forward to for the present.\n\nDoes that mean none of these models matter? Certainly they do. You don\u2019t get from version 2.0 to 3.0 without 2.1, 2.2, 2.2.1, and so on. And sometimes those advances are meaningful, address serious shortcomings, or expose unexpected vulnerabilities. We try to cover the interesting ones, but that\u2019s just a fraction of the full number. We\u2019re actually working on a piece now collecting all the models we think the ML-curious should be aware of, and it\u2019s on the order of a dozen.\n\nDon\u2019t worry: when a big one comes along, you\u2019ll know, and not just because TechCrunch is covering it. It\u2019s going to be as obvious to you as it is to us."}