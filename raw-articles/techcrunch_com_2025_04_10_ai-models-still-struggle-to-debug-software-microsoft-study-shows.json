{"url": "https://techcrunch.com/2025/04/10/ai-models-still-struggle-to-debug-software-microsoft-study-shows/", "title": "AI models still struggle to debug software, Microsoft study shows", "authors": ["Kyle Wiggers", "Ai Editor", "Zack Whittaker", "Rebecca Bellan", "Amanda Silberling", "Tim De Chant", "Connie Loizos", "Maxwell Zeff", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media"], "publication_date": "2025-04-10T00:00:00", "text": "AI models from OpenAI, Anthropic, and other top AI labs are increasingly being used to assist with programming tasks. Google CEO Sundar Pichai said in October that 25% of new code at the company is generated by AI, and Meta CEO Mark Zuckerberg has expressed ambitions to widely deploy AI coding models within the social media giant.\n\nYet even some of the best models today struggle to resolve software bugs that wouldn\u2019t trip up experienced devs.\n\nA new study from Microsoft Research, Microsoft\u2019s R&D division, reveals that models, including Anthropic\u2019s Claude 3.7 Sonnet and OpenAI\u2019s o3-mini, fail to debug many issues in a software development benchmark called SWE-bench Lite. The results are a sobering reminder that, despite bold pronouncements from companies like OpenAI, AI is still no match for human experts in domains such as coding.\n\nThe study\u2019s co-authors tested nine different models as the backbone for a \u201csingle prompt-based agent\u201d that had access to a number of debugging tools, including a Python debugger. They tasked this agent with solving a curated set of 300 software debugging tasks from SWE-bench Lite.\n\nAccording to the co-authors, even when equipped with stronger and more recent models, their agent rarely completed more than half of the debugging tasks successfully. Claude 3.7 Sonnet had the highest average success rate (48.4%), followed by OpenAI\u2019s o1 (30.2%), and o3-mini (22.1%).\n\nA chart from the study. The \u201crelative increase\u201d refers to the boost models got from being equipped with debugging tooling. Image Credits:Microsoft\n\nWhy the underwhelming performance? Some models struggled to use the debugging tools available to them and understand how different tools might help with different issues. The bigger problem, though, was data scarcity, according to the co-authors. They speculate that there\u2019s not enough data representing \u201csequential decision-making processes\u201d \u2014 that is, human debugging traces \u2014 in current models\u2019 training data.\n\n\u201cWe strongly believe that training or fine-tuning [models] can make them better interactive debuggers,\u201d wrote the co-authors in their study. \u201cHowever, this will require specialized data to fulfill such model training, for example, trajectory data that records agents interacting with a debugger to collect necessary information before suggesting a bug fix.\u201d\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\nThe findings aren\u2019t exactly shocking. Many studies have shown that code-generating AI tends to introduce security vulnerabilities and errors, owing to weaknesses in areas like the ability to understand programming logic. One recent evaluation of Devin, a popular AI coding tool, found that it could only complete three out of 20 programming tests.\n\nBut the Microsoft work is one of the more detailed looks yet at a persistent problem area for models. It likely won\u2019t dampen investor enthusiasm for AI-powered assistive coding tools, but with any luck, it\u2019ll make developers \u2014 and their higher-ups \u2014 think twice about letting AI run the coding show.\n\nFor what it\u2019s worth, a growing number of tech leaders have disputed the notion that AI will automate away coding jobs. Microsoft co-founder Bill Gates has said he thinks programming as a profession is here to stay. So has Replit CEO Amjad Masad, Okta CEO Todd McKinnon, and IBM CEO Arvind Krishna."}