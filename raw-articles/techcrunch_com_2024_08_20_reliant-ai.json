{"url": "https://techcrunch.com/2024/08/20/reliant-ai/", "title": "Reliant\u2019s paper-scouring AI takes on science\u2019s data drudgery", "authors": ["Devin Coldewey", "Writer", "Kyle Wiggers", "Julie Bort", "Maxwell Zeff", "Sarah Perez", "Marina Temkin", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media", "Min-Width"], "publication_date": "2024-08-20T00:00:00", "text": "AI models have proven capable of many things, but what tasks do we actually want them doing? Preferably drudgery \u2014 and there\u2019s plenty of that in research and academia. Reliant hopes to specialize in the kind of time-consuming data extraction work that\u2019s currently a specialty of tired grad students and interns.\n\n\u201cThe best thing you can do with AI is improve the human experience: reduce menial labor and let people do the things that are important to them,\u201d said CEO Karl Moritz Hermann. In the research world, where he and co-founders Marc Bellemare and Richard Schlegel have worked for years, literature review is one of the most common examples of this \u201cmenial labor.\u201d\n\nEvery paper cites previous and related work, but finding these sources in the sea of science is not easy. And some, like systematic reviews, cite or use data from thousands.\n\nFor one study, Hermann recalled, \u201cThe authors had to look at 3,500 scientific publications, and a lot of them ended up not being relevant. It\u2019s a ton of time spent extracting a tiny amount of useful information \u2014 this felt like something that really ought to be automated by AI.\u201d\n\nThey knew that modern language models could do it: One experiment put ChatGPT on the task and found that it was able to extract data with an 11% error rate. Like many things LLMs can do, it\u2019s impressive but nothing like what people actually need.\n\nImage Credits:Reliant AI\n\n\u201cThat\u2019s just not good enough,\u201d said Hermann. \u201cFor these knowledge tasks, menial as they may be, it\u2019s very important that you don\u2019t make mistakes.\u201d\n\nReliant\u2019s core product, Tabular, is based on an LLM in part (Llama 3.1), but augmented with other proprietary techniques, is considerably more effective. On the multi-thousand-study extraction above, they said it did the same task with zero errors.\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\nWhat that means is you dump a thousand documents in, say you want this, that, and the other data out of them, and Reliant pores through them and finds that information \u2014 whether it\u2019s perfectly labeled and structured or (far more likely) it isn\u2019t. Then it pops all that data and any analyses you wanted done into a nice UI so you can dive down into individual cases.\n\n\u201cOur users need to be able to work with all the data all at once, and we\u2019re building features to allow them to edit the data that\u2019s there, or go from the data to the literature; we see our role as helping the users find where to spend their attention,\u201d Hermann said.\n\nImage Credits:Reliant\n\nThis tailored and effective application of AI \u2014 not as splashy as a digital friend but almost certainly much more viable \u2014 could accelerate science across a number of highly technical domains. Investors have taken note, funding an $11.3 million seed round; Tola Capital and Inovia Capital led the round, with angel Mike Volpi participating.\n\nLike any application of AI, Reliant\u2019s tech is very compute-intensive, which is why the company has bought its own hardware rather than renting it a la carte from one of the big providers. Going in-house with hardware offers both risk and reward: You have to make these expensive machines pay for themselves, but you get the chance to crack open the problem space with dedicated compute.\n\n\u201cOne thing that we\u2019ve found is it\u2019s very challenging to give a good answer if you have limited time to give that answer,\u201d Hermann explained \u2014 for instance, if a scientist asks the system to perform a novel extraction or analysis task on a hundred papers. It can be done quickly, or well, but not both \u2014 unless they predict what users might ask and figure out the answer, or something like it, ahead of time.\n\n\u201cThe thing is, a lot of people have the same questions, so we can find the answers before they ask, as a starting point,\u201d said Bellemare, the startup\u2019s chief science officer. \u201cWe can distill 100 pages of text into something else, that may not be exactly what you want, but it\u2019s easier for us to work with.\u201d\n\nThink about it this way: If you were going to extract the meaning from a thousand novels, would you wait until someone asked for the characters\u2019 names to go through and grab them? Or would you just do that work ahead of time (along with things like locations, dates, relationships, etc.) knowing the data would likely be wanted? Certainly the latter \u2014 if you had the compute to spare.\n\nThis pre-extraction also gives the models time to resolve the inevitable ambiguities and assumptions found in different scientific domains. When one metric \u201cindicates\u201d another, it may not mean the same thing in pharmaceuticals as it does in pathology or clinical trials. Not only that, but language models tend to give different outputs depending on how they\u2019re asked certain questions. So Reliant\u2019s job has been to turn ambiguity into certainty \u2014 \u201cand this is something you can only do if you\u2019re willing to invest in a particular science or domain,\u201d Hermann noted.\n\nAs a company, Reliant\u2019s first focus is on establishing that the tech can pay for itself before attempting anything more ambitious. \u201cIn order to make interesting progress, you have to have a big vision but you also need to start with something concrete,\u201d said Hermann. \u201cFrom a startup survival point of view, we focus on for-profit companies, because they give us money to pay for our GPUs. We\u2019re not selling this at a loss to customers.\u201d\n\nOne might expect the firm to feel the heat from companies like OpenAI and Anthropic, which are pouring money into handling more structured tasks like database management and coding, or from implementation partners like Cohere and Scale. But Bellemare was optimistic: \u201cWe\u2019re building this on a groundswell \u2014 any improvement in our tech stack is great for us. The LLM is one of maybe eight large machine learning models in there \u2014 the others are fully proprietary to us, made from scratch on data propriety to us.\u201d\n\nThe transformation of the biotech and research industry into an AI-driven one is certainly only beginning and may be fairly patchwork for years to come. But Reliant seems to have found a strong footing to start from.\n\n\u201cIf you want the 95% solution, and you just apologize profusely to one of your customers once in a while, great,\u201d said Hermann. \u201cWe\u2019re for where precision and recall really matter, and where mistakes really matter. And frankly, that\u2019s enough; we\u2019re happy to leave the rest to others.\u201d\n\n(This story originally had Hermann\u2019s name incorrect \u2014 my own error, I have changed it throughout.)"}