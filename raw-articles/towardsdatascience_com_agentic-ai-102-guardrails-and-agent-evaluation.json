{"url": "https://towardsdatascience.com/agentic-ai-102-guardrails-and-agent-evaluation/", "title": "Agentic AI 102: Guardrails and Agent Evaluation", "authors": ["Gustavo Santos", "Shaw Talebi", "Srijanie Dey", "Giuseppe Scalamogna", "Roberta Rocca", "Mark Chen", "Mariano Kamp", "Hennie De Harder", ".Wp-Block-Post-Author-Name Box-Sizing Border-Box"], "publication_date": "2025-05-16T19:09:26+00:00", "text": "In the first post of this series (Agentic AI 101: Starting Your Journey Building AI Agents), we talked about the fundamentals of creating AI Agents and introduced concepts like reasoning, memory, and tools.\n\nOf course, that first post touched only the surface of this new area of the data industry. There is so much more that can be done, and we are going to learn more along the way in this series.\n\nSo, it is time to take one step further.\n\nIn this post, we will cover three topics:\n\nGuardrails: these are safe blocks that prevent a Large Language Model (LLM) from responding about some topics. Agent Evaluation: Have you ever thought about how accurate the responses from LLM are? I bet you did. So we will see the main ways to measure that. Monitoring: We will also learn about the built-in monitoring app in Agno\u2019s framework.\n\nWe shall begin now.\n\nGuardrails\n\nOur first topic is the simplest, in my opinion. Guardrails are rules that will keep an AI agent from responding to a given topic or list of topics.\n\nI believe there is a good chance that you have ever asked something to ChatGPT or Gemini and received a response like \u201cI can\u2019t talk about this topic\u201d, or \u201cPlease consult a professional specialist\u201d, something like that. Usually, that occurs with sensitive topics like health advice, psychological conditions, or financial advice.\n\nThose blocks are safeguards to prevent people from hurting themselves, harming their health, or their pockets. As we know, LLMs are trained on massive amounts of text, ergo inheriting a lot of bad content with it, which could easily lead to bad advice in those areas for people. And I didn\u2019t even mention hallucinations!\n\nThink about how many stories there are of people who lost money by following investment tips from online forums. Or how many people took the wrong medicine because they read about it on the internet.\n\nWell, I guess you got the point. We must prevent our agents from talking about certain topics or taking certain actions. For that, we will use guardrails.\n\nThe best framework I found to impose those blocks is Guardrails AI [1]. There, you will see a hub full of predefined rules that a response must follow in order to pass and be displayed to the user.\n\nTo get started quickly, first go to this link [2] and get an API key. Then, install the package. Next, type the guardrails setup command. It will ask you a couple of questions that you can respond n (for No), and it will ask you to enter the API Key generated.\n\npip install guardrails-ai guardrails configure\n\nOnce that is completed, go to the Guardrails AI Hub [3] and choose one that you need. Every guardrail has instructions on how to implement it. Basically, you install it via the command line and then use it like a module in Python.\n\nFor this example, we\u2019re choosing one called Restrict to Topic [4], which, as its name says, lets the user talk only about what\u2019s in the list. So, go back to the terminal and install it using the code below.\n\nguardrails hub install hub://tryolabs/restricttotopic\n\nNext, let\u2019s open our Python script and import some modules.\n\n# Imports from agno.agent import Agent from agno.models.google import Gemini import os # Import Guard and Validator from guardrails import Guard from guardrails.hub import RestrictToTopic\n\nNext, we create the guard. We will restrict our agent to talk only about sports or the weather. And we are restricting it to talk about stocks.\n\n# Setup Guard guard = Guard().use( RestrictToTopic( valid_topics=[\"sports\", \"weather\"], invalid_topics=[\"stocks\"], disable_classifier=True, disable_llm=False, on_fail=\"filter\" ) )\n\nNow we can run the agent and the guard.\n\n# Create agent agent = Agent( model= Gemini(id=\"gemini-1.5-flash\", api_key = os.environ.get(\"GEMINI_API_KEY\")), description= \"An assistant agent\", instructions= [\"Be sucint. Reply in maximum two sentences\"], markdown= True ) # Run the agent response = agent.run(\"What's the ticker symbol for Apple?\").content # Run agent with validation validation_step = guard.validate(response) # Print validated response if validation_step.validation_passed: print(response) else: print(\"Validation Failed\", validation_step.validation_summaries[0].failure_reason)\n\nThis is the response when we ask about a stock symbol.\n\nValidation Failed Invalid topics found: ['stocks']\n\nIf I ask about a topic that is not on the valid_topics list, I will also see a block.\n\n\"What's the number one soda drink?\" Validation Failed No valid topic was found.\n\nFinally, let\u2019s ask about sports.\n\n\"Who is Michael Jordan?\" Michael Jordan is a former professional basketball player widely considered one of the greatest of all time. He won six NBA championships with the Chicago Bulls.\n\nAnd we saw a response this time, as it is a valid topic.\n\nLet\u2019s move on to the evaluation of agents now.\n\nAgent Evaluation\n\nSince I started studying LLMs and Agentic Ai, one of my main questions has been about model evaluation. Unlike traditional Data Science Modeling, where you have structured metrics that are adequate for each case, for AI Agents, this is more blurry.\n\nFortunately, the developer community is pretty quick in finding solutions for almost everything, and so they created this nice package for LLMs evaluation: deepeval .\n\nDeepEval [5] is a library created by Confident AI that gathers many methods to evaluate LLMs and AI Agents. In this section, let\u2019s learn a couple of the main methods, just so we can build some intuition on the subject, and also because the library is quite extensive.\\\n\nThe first evaluation is the most basic we can use, and it is called G-Eval . As AI tools like ChatGPT become more common in everyday tasks, we have to make sure they\u2019re giving helpful and accurate responses. That\u2019s where G-Eval from the DeepEval Python package comes in.\n\nG-Eval is like a smart reviewer that uses another AI model to evaluate how well a chatbot or AI assistant is performing. For example. My agent runs Gemini, and I am using OpenAI to assess it. This method takes a more advanced approach than a human one by asking an AI to \u201cgrade\u201d another AI\u2019s answers based on things like relevance, correctness, and clarity.\n\nIt\u2019s a nice way to test and improve generative AI systems in a more scalable way. Let\u2019s quickly code an example. We will import the modules, create a prompt, a simple chat agent, and ask it about a description of the weather for the month of May in NYC.\n\n# Imports from agno.agent import Agent from agno.models.google import Gemini import os # Evaluation Modules from deepeval.test_case import LLMTestCase, LLMTestCaseParams from deepeval.metrics import GEval # Prompt prompt = \"Describe the weather in NYC for May\" # Create agent agent = Agent( model= Gemini(id=\"gemini-1.5-flash\", api_key = os.environ.get(\"GEMINI_API_KEY\")), description= \"An assistant agent\", instructions= [\"Be sucint\"], markdown= True, monitoring= True ) # Run agent response = agent.run(prompt) # Print response print(response.content)\n\nIt responds: \u201cMild, with average highs in the 60s\u00b0F and lows in the 50s\u00b0F. Expect some rain\u201c.\n\nNice. Seems pretty good to me.\n\nBut how can we put a number on it and show a potential manager or client how our agent is doing?\n\nHere is how:\n\nCreate a test case passing the prompt and the response to the LLMTestCase class. Create a metric. We will use the method GEval and add a prompt for the model to test it for coherence, and then I give it the meaning of what coherence is to me. Give the output as evaluation_params . Run the measure method and get the score and reason from it.\n\n# Test Case test_case = LLMTestCase(input=prompt, actual_output=response) # Setup the Metric coherence_metric = GEval( name=\"Coherence\", criteria=\"Coherence. The agent can answer the prompt and the response makes sense.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT] ) # Run the metric coherence_metric.measure(test_case) print(coherence_metric.score) print(coherence_metric.reason)\n\nThe output looks like this.\n\n0.9 The response directly addresses the prompt about NYC weather in May, maintains logical consistency, flows naturally, and uses clear language. However, it could be slightly more detailed.\n\n0.9 seems pretty good, given that the default threshold is 0.5.\n\nIf you want to check the logs, use this next snippet.\n\n# Check the logs print(coherence_metric.verbose_logs)\n\nHere\u2019s the response.\n\nCriteria: Coherence. The agent can answer the prompt and the response makes sense. Evaluation Steps: [ \"Assess whether the response directly addresses the prompt; if it aligns, it scores higher on coherence.\", \"Evaluate the logical flow of the response; responses that present ideas in a clear, organized manner rank better in coherence.\", \"Consider the relevance of examples or evidence provided; responses that include pertinent information enhance their coherence.\", \"Check for clarity and consistency in terminology; responses that maintain clear language without contradictions achieve a higher coherence rating.\" ]\n\nVery nice. Now let us learn about another interesting use case, which is the evaluation of task completion for AI Agents. Elaborating a little more, how our agent is doing when it is requested to perform a task, and how much of it the agent can deliver.\n\nFirst, we are creating a simple agent that can access Wikipedia and summarize the topic of the query.\n\n# Imports from agno.agent import Agent from agno.models.google import Gemini from agno.tools.wikipedia import WikipediaTools import os from deepeval.test_case import LLMTestCase, ToolCall from deepeval.metrics import TaskCompletionMetric from deepeval import evaluate # Prompt prompt = \"Search wikipedia for 'Time series analysis' and summarize the 3 main points\" # Create agent agent = Agent( model= Gemini(id=\"gemini-2.0-flash\", api_key = os.environ.get(\"GEMINI_API_KEY\")), description= \"You are a researcher specialized in searching the wikipedia.\", tools= [WikipediaTools()], show_tool_calls= True, markdown= True, read_tool_call_history= True ) # Run agent response = agent.run(prompt) # Print response print(response.content)\n\nThe result looks very good. Let\u2019s evaluate it using the TaskCompletionMetric class.\n\n# Create a Metric metric = TaskCompletionMetric( threshold=0.7, model=\"gpt-4o-mini\", include_reason=True ) # Test Case test_case = LLMTestCase( input=prompt, actual_output=response.content, tools_called=[ToolCall(name=\"wikipedia\")] ) # Evaluate evaluate(test_cases=[test_case], metrics=[metric])\n\nOutput, including the agent\u2019s response.\n\n====================================================================== Metrics Summary - \u2705 Task Completion (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o-mini, reason: The system successfully searched for 'Time series analysis' on Wikipedia and provided a clear summary of the 3 main points, fully aligning with the user's goal., error: None) For test case: - input: Search wikipedia for 'Time series analysis' and summarize the 3 main points - actual output: Here are the 3 main points about Time series analysis based on the Wikipedia search: 1. **Definition:** A time series is a sequence of data points indexed in time order, often taken at successive, equally spaced points in time. 2. **Applications:** Time series analysis is used in various fields like statistics, signal processing, econometrics, weather forecasting, and more, wherever temporal measurements are involved. 3. **Purpose:** Time series analysis involves methods for extracting meaningful statistics and characteristics from time series data, and time series forecasting uses models to predict future values based on past observations. - expected output: None - context: None - retrieval context: None ====================================================================== Overall Metric Pass Rates Task Completion: 100.00% pass rate ====================================================================== \u2713 Tests finished \ud83c\udf89! Run 'deepeval login' to save and analyze evaluation results on Confident AI.\n\nOur agent passed the test with honor: 100%!\n\nYou can learn much more about the DeepEval library in this link [8].\n\nFinally, in the next section, we will learn the capabilities of Agno\u2019s library for monitoring agents.\n\nAgent Monitoring\n\nLike I told you in my previous post [9], I chose Agno to learn more about Agentic AI. Just to be clear, this is not a sponsored post. It is just that I think this is the best option for those starting their journey learning about this topic.\n\nSo, one of the cool things we can take advantage of using Agno\u2019s framework is the app they make available for model monitoring.\n\nTake this agent that can search the internet and write Instagram posts, for example.\n\n# Imports import os from agno.agent import Agent from agno.models.google import Gemini from agno.tools.file import FileTools from agno.tools.googlesearch import GoogleSearchTools # Topic topic = \"Healthy Eating\" # Create agent agent = Agent( model= Gemini(id=\"gemini-1.5-flash\", api_key = os.environ.get(\"GEMINI_API_KEY\")), description= f\"\"\"You are a social media marketer specialized in creating engaging content. Search the internet for 'trending topics about {topic}' and use them to create a post.\"\"\", tools=[FileTools(save_files=True), GoogleSearchTools()], expected_output=\"\"\"A short post for instagram and a prompt for a picture related to the content of the post. Don't use emojis or special characters in the post. If you find an error in the character encoding, remove the character before saving the file. Use the template: - Post - Prompt for the picture Save the post to a file named 'post.txt'.\"\"\", show_tool_calls=True, monitoring=True) # Writing and saving a file agent.print_response(\"\"\"Write a short post for instagram with tips and tricks that positions me as an authority in {topic}.\"\"\", markdown=True)\n\nTo monitor its performance, follow these steps:\n\nGo to https://app.agno.com/settings and get an API Key. Open a terminal and type ag setup . If it is the first time, it might ask for the API Key. Copy and Paste it in the terminal prompt. You will see the Dashboard tab open in your browser. If you want to monitor your agent, add the argument monitoring=True . Run your agent. Go to the Dashboard on the web browser. Click on Sessions. As it is a single agent, you will see it under the tab Agents on the top portion of the page.\n\nAgno Dashboard after running the agent. Image by the author.\n\nThe cools features we can see there are:\n\nInfo about the model\n\nThe response\n\nTools used\n\nTokens consumed\n\nThis is the resulting token consumption while saving the file. Image by the author.\n\nPretty neat, huh?\n\nThis is useful for us to know where the agent is spending more or less tokens, and where it is taking more time to perform a task, for example.\n\nWell, let\u2019s wrap up then.\n\nBefore You Go\n\nWe have learned a lot in this second round. In this post, we covered:\n\nGuardrails for AI are essential safety measures and ethical guidelines implemented to prevent unintended harmful outputs and ensure responsible AI behavior.\n\nare essential safety measures and ethical guidelines implemented to prevent unintended harmful outputs and ensure responsible AI behavior. Model evaluation , exemplified by GEval for broad assessment and TaskCompletion with DeepEval for agents output quality, is crucial for understanding AI capabilities and limitations.\n\n, exemplified by for broad assessment and with DeepEval for agents output quality, is crucial for understanding AI capabilities and limitations. Model monitoring with Agno\u2019s app, including tracking token usage and response time, which is vital for managing costs, ensuring performance, and identifying potential issues in deployed AI systems.\n\nContact & Follow Me\n\nIf you liked this content, find more of my work in my website.\n\nhttps://gustavorsantos.me\n\nGitHub Repository\n\nhttps://github.com/gurezende/agno-ai-labs\n\nReferences\n\n[1. Guardrails Ai] https://www.guardrailsai.com/docs/getting_started/guardrails_server\n\n[2. Guardrails AI Auth Key] https://hub.guardrailsai.com/keys\n\n[3. Guardrails AI Hub] https://hub.guardrailsai.com/\n\n[4. Guardrails Restrict to Topic] https://hub.guardrailsai.com/validator/tryolabs/restricttotopic\n\n[5. DeepEval.] https://www.deepeval.com/docs/getting-started\n\n[6. DataCamp \u2013 DeepEval Tutorial] https://www.datacamp.com/tutorial/deepeval\n\n[7. DeepEval. TaskCompletion] https://www.deepeval.com/docs/metrics-task-completion\n\n[8. Llm Evaluation Metrics: The Ultimate LLM Evaluation Guide] https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation\n\n[9. Agentic AI 101: Starting Your Journey Building AI Agents] https://towardsdatascience.com/agentic-ai-101-starting-your-journey-building-ai-agents/"}