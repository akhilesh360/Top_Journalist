{"url": "https://techcrunch.com/2023/12/04/animate-anyone-heralds-the-approach-of-full-motion-deepfakes/", "title": "\u2018Animate Anyone\u2019 heralds the approach of full-motion deepfakes", "authors": ["Devin Coldewey", "Writer", "Kyle Wiggers", "Amanda Silberling", "Rebecca Bellan", "Lorenzo Franceschi-Bicchierai", "Sean O'Kane", "Kirsten Korosec", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media"], "publication_date": "2023-12-04T00:00:00", "text": "As if still-image deepfakes aren\u2019t bad enough, we may soon have to contend with generated videos of anyone who dares to put a photo of themselves online: with Animate Anyone, bad actors can puppeteer people better than ever.\n\nThe new generative video technique was developed by researchers at Alibaba Group\u2019s Institute for Intelligent Computing. It\u2019s a big step forward from previous image-to-video systems like DisCo and DreamPose, which were impressive all the way back in summer but are now ancient history.\n\nWhat Animate Anyone can do is not by any means unprecedented, but has passed that difficult space between \u201cjanky academic experiment\u201d and \u201cgood enough if you don\u2019t look closely.\u201d As we all know, the next stage is just plain \u201cgood enough,\u201d where people won\u2019t even bother looking closely because they assume it\u2019s real. That\u2019s where still images and text conversation are currently, wreaking havoc on our sense of reality.\n\nImage-to-video models like this one start by extracting details, like facial feature, patterns and pose, from a reference image like a fashion photo of a model wearing a dress for sale. Then a series of images is created where those details are mapped onto very slightly different poses, which can be motion-captured or themselves extracted from another video.\n\nPrevious models showed that this was possible to do, but there were lots of issues. Hallucination was a big problem, as the model has to invent plausible details like how a sleeve or hair might move when a person turns. This leads to a lot of really weird imagery, making the resulting video far from convincing. But the possibility remained, and Animate Anyone is much improved, though still far from perfect.\n\nThe technical specifics of the new model are beyond most, but the paper emphasizes a new intermediate step that \u201cenables the model to comprehensively learn the relationship with the reference image in a consistent feature space, which significantly contributes to the improvement of appearance details preservation.\u201d By improving the retention of basic and fine details, generated images down the line have a stronger ground truth to work with and turn out a lot better.\n\nThey show off their results in a few contexts. Fashion models take on arbitrary poses without deforming or the clothing losing its pattern. A 2D anime figure comes to life and dances convincingly. Lionel Messi makes a few generic movements.\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\nThey\u2019re far from perfect \u2014 especially about the eyes and hands, which pose particular trouble for generative models. And the poses that are best represented are those closest to the original; if the person turns around, for instance, the model struggles to keep up. But it\u2019s a huge leap over the previous state of the art, which produced way more artifacts or completely lost important details like the color of a person\u2019s hair or their clothing.\n\nIt\u2019s unnerving thinking that given a single good-quality image of you, a malicious actor (or producer) could make you do just about anything, and combined with facial animation and voice capture tech, they could also make you express anything at the same time. For now, the tech is too complex and buggy for general use, but things don\u2019t tend to stay that way for long in the AI world.\n\nAt least the team isn\u2019t unleashing the code into the world just yet. Though they have a GitHub page, the developers write: \u201cwe are actively working on preparing the demo and code for public release. Although we cannot commit to a specific release date at this very moment, please be certain that the intention to provide access to both the demo and our source code is firm.\u201d\n\nWill all hell break loose when the internet is suddenly flooded with dancefakes? We\u2019ll find out, and probably sooner than we would like."}