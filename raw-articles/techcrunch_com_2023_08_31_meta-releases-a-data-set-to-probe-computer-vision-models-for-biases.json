{"url": "https://techcrunch.com/2023/08/31/meta-releases-a-data-set-to-probe-computer-vision-models-for-biases/", "title": "Meta releases a dataset to probe computer vision models for biases", "authors": ["Kyle Wiggers", "Ai Editor", "Amanda Silberling", "Rebecca Bellan", "Lorenzo Franceschi-Bicchierai", "Sean O'Kane", "Kirsten Korosec", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media", "Min-Width"], "publication_date": "2023-08-31T00:00:00", "text": "Continuing on its open source tear, Meta today released a new AI benchmark, FACET, designed to evaluate the \u201cfairness\u201d of AI models that classify and detect things in photos and videos, including people.\n\nMade up of 32,000 images containing 50,000 people labeled by human annotators, FACET \u2014 a tortured acronym for \u201cFAirness in Computer Vision EvaluaTion\u201d \u2014 accounts for classes related to occupations and activities like \u201cbasketball player,\u201d \u201cdisc jockey\u201d and \u201cdoctor\u201d in addition to demographic and physical attributes, allowing for what Meta describes as \u201cdeep\u201d evaluations of biases against those classes.\n\n\u201cBy releasing FACET, our goal is to enable researchers and practitioners to perform similar benchmarking to better understand the disparities present in their own models and monitor the impact of mitigations put in place to address fairness concerns,\u201d Meta wrote in a blog post shared with TechCrunch. \u201cWe encourage researchers to use FACET to benchmark fairness across other vision and multimodal tasks.\u201d\n\nCertainly, benchmarks to probe for biases in computer vision algorithms aren\u2019t new. Meta itself released one several years ago to surface age, gender and skin tone discrimination in both computer vision and audio machine learning models. And a number of studies have been conducted on computer vision models to determine whether they\u2019re biased against certain demographic groups. (Spoiler alert: they usually are.)\n\nThen, there\u2019s the fact that Meta doesn\u2019t have the best track record when it comes to responsible AI.\n\nLate last year, Meta was forced to pull an AI demo after it wrote racist and inaccurate scientific literature. Reports have characterized the company\u2019s AI ethics team as largely toothless and the anti-AI-bias tools it\u2019s released as \u201ccompletely insufficient.\u201d Meanwhile, academics have accused Meta of exacerbating socioeconomic inequalities in its ad-serving algorithms and of showing a bias against Black users in its automated moderation systems.\n\nBut Meta claims FACET is more thorough than any of the computer vision bias benchmarks that came before it \u2014 able to answer questions like \u201cAre models better at classifying people as skateboarders when their perceived gender presentation has more stereotypically male attributes?\u201d and \u201cAre any biases magnified when the person has coily hair compared to straight hair?\u201d\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\nTo create FACET, Meta had the aforementioned annotators label each of the 32,000 images for demographic attributes (e.g. the pictured person\u2019s perceived gender presentation and age group), additional physical attributes (e.g. skin tone, lighting, tattoos, headwear and eyewear, hairstyle and facial hair, etc.) and classes. They combined these labels with other labels for people, hair and clothing taken from Segment Anything 1 Billion, a Meta-designed dataset for training computer vision models to \u201csegment,\u201d or isolate, objects and animals from images.\n\nThe images from FACET were sourced from Segment Anything 1 Billion, Meta tells me, which in turn were purchased from a \u201cphoto provider.\u201d But it\u2019s unclear whether the people pictured in them were made aware that the pictures would be used for this purpose. And \u2014 at least in the blog post \u2014 it\u2019s not clear how Meta recruited the annotator teams, and what wages they were paid.\n\nHistorically and even today, many of the annotators employed to label datasets for AI training and benchmarking come from developing countries and have incomes far below the U.S.\u2019 minimum wage. Just this week, The Washington Post reported that Scale AI, one of the largest and best-funded annotation firms, has paid workers at extremely low rates, routinely delayed or withheld payments and provided few channels for workers to seek recourse.\n\nIn a white paper describing how FACET came together, Meta says that the annotators were \u201ctrained experts\u201d sourced from \u201cseveral geographic regions\u201d including North America (United States), Latin American (Colombia), Middle East (Egypt), Africa (Kenya), Southeast Asia (Philippines) and East Asia (Taiwan). Meta used a \u201cproprietary annotation platform\u201d from a third-party vendor, it says, and annotators were compensated \u201cwith an hour wage set per country.\u201d\n\nSetting aside FACET\u2019s potentially problematic origins, Meta says that the benchmark can be used to probe classification, detection, \u201cinstance segmentation\u201d and \u201cvisual grounding\u201d models across different demographic attributes.\n\nAs a test case, Meta applied FACET to its own DINOv2 computer vision algorithm, which as of this week is available for commercial use. FACET uncovered several biases in DINOv2, Meta says, including a bias against people with certain gender presentations and a likelihood to stereotypically identify pictures of women as \u201cnurses.\u201d\n\n\u201cThe preparation of DINOv2\u2019s pre-training dataset may have inadvertently replicated the biases of the reference datasets selected for curation,\u201d Meta wrote in the blog post. \u201cWe plan to address these potential shortcomings in future work and believe that image-based curation could also help avoid the perpetuation of potential biases arising from the use of search engines or text supervision.\u201d\n\nNo benchmark is perfect. And Meta, to its credit, acknowledges that FACET might not sufficiently capture real-world concepts and demographic groups. It also notes that many depictions of professions in the dataset might\u2019ve changed since FACET was created. For example, most doctors and nurses in FACET, photographed during the COVID-19 pandemic, are wearing more personal protective equipment than they would\u2019ve before the health crises.\n\n\u201cAt this time we do not plan to have updates for this dataset,\u201d Meta writes in the whitepaper. \u201cWe will allow users to flag any images that may be objectionable content, and remove objectionable content if found.\u201d\n\nIn addition to the dataset itself, Meta has made available a web-based dataset explorer tool. To use it and the dataset, developers must agree not to train computer vision models on FACET \u2014 only evaluate, test and benchmark them."}