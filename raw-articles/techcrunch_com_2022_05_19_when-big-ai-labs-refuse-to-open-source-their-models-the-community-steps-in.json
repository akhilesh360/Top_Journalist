{"url": "https://techcrunch.com/2022/05/19/when-big-ai-labs-refuse-to-open-source-their-models-the-community-steps-in/", "title": "How the AI community open-sources proprietary models", "authors": ["Kyle Wiggers", "Ai Editor", "Amanda Silberling", "Rebecca Bellan", "Lorenzo Franceschi-Bicchierai", "Sean O'Kane", "Kirsten Korosec", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media", "Min-Width"], "publication_date": "2022-05-19T00:00:00", "text": "Benchmarks are as important a measure of progress in AI as they are for the rest of the software industry. But when the benchmark results come from corporations, secrecy very often prevents the community from verifying them.\n\nFor example, OpenAI granted Microsoft, with which it has a commercial relationship, the exclusive licensing rights to its powerful GPT-3 language model. Other organizations say that the code they use to develop systems is dependent on impossible-to-release internal tooling and infrastructure or uses copyrighted datasets. While motivations can be ethical in nature \u2014 OpenAI initially declined to release GPT-2, GPT-3\u2019s predecessor, out of concerns that it might be misused \u2014 the effect is the same. Without the necessary code, it\u2019s far harder for third-party researchers to verify an organization\u2019s claims.\n\n\u201cThis isn\u2019t really a sufficient alternative to good industry open-source practices,\u201d Harvard computer science PhD candidate Gustaf Ahdritz told TechCrunch via email. Ahdritz is one of the lead developers of OpenFold, an open source version of DeepMind\u2019s protein structure-predicting AlphaFold 2. \u201cIt\u2019s difficult to do all of the science one might like to do with the code DeepMind did release.\u201d\n\nSome researchers go so far as to say that withholding a system\u2019s code \u201cundermines its scientific value.\u201d In October 2020, a rebuttal published in the journal Nature took issue with a cancer-predicting system trained by Google Health, the branch of Google focused on health-related research. The co-authors noted that Google withheld key technical details, including a description of how the system was developed, which could significantly impact its performance.\n\nIn lieu of change, some members of the AI community, like Ahdritz, have made it their mission to open source the systems themselves. Working from technical papers, these researchers painstakingly try to recreate the systems, either from scratch or building on the fragments of publicly available specifications.\n\nOpenFold is one such effort. Begun shortly after DeepMind announced AlphaFold 2, the goal is to verify that AlphaFold 2 can be reproduced from scratch and make available components of the system that might be useful elsewhere, according to Ahdritz.\n\n\u201cWe trust that DeepMind provided all the necessary details, but \u2026 we don\u2019t have [concrete] proof of that, and so this effort is key to providing that trail and allowing others to build on it,\u201d Ahdritz said. \u201cMoreover, originally, certain AlphaFold components were under a non-commercial license. Our components and data \u2014 DeepMind still hasn\u2019t published their full training data \u2014 are going to be completely open-source, enabling industry adoption.\u201d\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\nOpenFold isn\u2019t the only project of its kind. Elsewhere, loosely affiliated groups within the AI community are attempting implementations of OpenAI\u2019s code-generating Codex and art-creating DALL-E, DeepMind\u2019s chess-playing AlphaZero and even AlphaStar, a DeepMind system designed to play the real-time strategy game StarCraft 2. Among the more successful are EleutherAI and AI startup Hugging Face\u2019s BigScience, open research efforts that aim to deliver the code and datasets needed to run a model comparable (though not identical) to GPT-3.\n\nPhilip Wang, a prolific member of the AI community who maintains a number of open source implementations on GitHub, including one of OpenAI\u2019s DALL-E, posits that open sourcing these systems reduces the need for researchers to duplicate their efforts.\n\n\u201cWe read the latest AI studies, like any other researcher in the world. But instead of replicating the paper in a silo, we implement it open source,\u201d Wang said. \u201cWe are in an interesting place at the intersection of information science and industry. I think open source is not one-sided and benefits everybody in the end. It also appeals to the broader vision of truly democratized AI not beholden to shareholders.\u201d\n\nBrian Lee and Andrew Jackson, two Google employees, worked together to create MiniGo, a replication of AlphaZero. While not affiliated with the official project, Lee and Jackson \u2014 being at Google, DeepMind\u2019s initial parent company \u2014 had the advantage of access to certain proprietary resources.\n\n\u201c[Working backward from papers is] like navigating before we had GPS,\u201d Lee, a research engineer at Google Brain, told TechCrunch via email. \u201cThe instructions talk about landmarks you ought to see, how long you ought to go in a certain direction, which fork to take at a critical juncture. There\u2019s enough detail for the experienced navigator to find their way, but if you don\u2019t know how to read a compass, you\u2019ll be hopelessly lost. You won\u2019t retrace the steps exactly, but you\u2019ll end up in the same place.\u201d\n\nThe developers behind these initiatives, Ahdritz and Jackson included, say that they\u2019ll not only help to demonstrate whether the systems work as advertised but enable new applications and better hardware support. Systems from large labs and companies like DeepMind, OpenAI, Microsoft, Amazon and Meta are typically trained on expensive, proprietary data center servers with far more compute power than the average workstation, adding to the hurdles of open sourcing them.\n\n\u201cTraining new variants of AlphaFold could lead to new applications beyond protein structure prediction, which is not possible with DeepMind\u2019s original code release because it lacked the training code \u2014 for example, predicting how drugs bind proteins, how proteins move, and how proteins interact with other biomolecules,\u201d Ahdritz said. \u201cThere are dozens of high-impact applications that require training new variants of AlphaFold or integrating parts of AlphaFold into larger models, but the lack of training code prevents all of them.\u201d\n\n\u201cThese open-source efforts do a lot to disseminate the \u2018working knowledge\u2019 about how these systems can behave in non-academic settings,\u201d Jackson added. \u201cThe amount of compute needed to reproduce the original results [for AlphaZero] is pretty high. I don\u2019t remember the number off the top of my head, but it involved running about a thousand GPUs for a week. We were in a pretty unique position to be able to help the community try these models with our early access to the Google Cloud Platform\u2019s TPU product, which was not yet publicly available.\u201d\n\nImplementing proprietary systems in open source is fraught with challenges, especially when there\u2019s little public information to go on. Ideally, the code is available in addition to the data set used to train the system and what are called weights, which are responsible for transforming data fed to the system into predictions. But this isn\u2019t often the case.\n\nFor example, in developing OpenFold, Ahdritz and team had to gather information from the official materials and reconcile the differences between different sources, including the source code, supplemental code and presentations that DeepMind researchers gave early on. Ambiguities in steps like data prep and training code led to false starts, while a lack of hardware resources necessitated design compromises.\n\n\u201cWe only really get a handful of tries to get this right, lest this drag on indefinitely. These things have so many computationally intensive stages that a tiny bug can greatly set us back, such that we had to retrain the model and also regenerate lots of training data,\u201d Ahdritz said. \u201cSome technical details that work very well for [DeepMind] don\u2019t work as easily for us because we have different hardware \u2026 In addition, ambiguity about what details are critically important and which ones are selected without much thought makes it hard to optimize or tweak anything and locks us in to whatever (sometimes awkward) choices were made in the original system.\u201d\n\nSo, do the labs behind the proprietary systems, like OpenAI, care that their work is being reverse-engineered and even used by startups to launch competing services? Evidently not. Ahdritz says the fact that DeepMind in particular releases so many details about its systems suggests it implicitly endorses the efforts, even if it hasn\u2019t said so publicly.\n\n\u201cWe haven\u2019t received any clear indication that DeepMind disapproves or approves of this effort,\u201d Ahdritz said. \u201cBut certainly, no one has tried to stop us.\u201d"}