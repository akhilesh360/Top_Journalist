{"url": "https://towardsdatascience.com/googles-alphaevolve-is-evolving-new-algorithms-and-it-could-be-a-game-changer/", "title": "Google\u2019s AlphaEvolve Is Evolving New Algorithms \u2014 And It Could Be a Game Changer", "authors": ["Luciano Abriata", "Shreya Rao", "Dr. Robert K\u00fcbler", "Piero Paialunga", "Shaw Talebi", "Luigi Battistoni", "Tds Editors", ".Wp-Block-Post-Author-Name Box-Sizing Border-Box"], "publication_date": "2025-05-16T00:43:22+00:00", "text": "AlphaEvolve imagined as a genetic algorithm coupled to a large language model. Picture created by the author using various tools including Dall-E3 via ChatGPT.\n\nModels have undeniably revolutionized how many of us approach coding, but they\u2019re often more like a super-powered intern than a seasoned architect. Errors, bugs and hallucinations happen all the time, and it might even happen that the code runs well but\u2026 it\u2019s not doing exactly what we wanted.\n\nNow, imagine an AI that doesn\u2019t just write code based on what it\u2019s seen, but actively evolves it. To a first surprise, this means you increase the chances of getting the right code written; however, it goes far beyond: Google showed that it can also use such AI methodology to discover new algorithms that are faster, more efficient, and sometimes, entirely new.\n\nI\u2019m talking about AlphaEvolve, the recent bombshell from Google DeepMind. Let me say it again: it isn\u2019t just another code generator, but rather a system that generates and evolves code, allowing it to discover new algorithms. Powered by Google\u2019s formidable Gemini models (that I intend to cover soon, because I\u2019m amazed at their power!), AlphaEvolve could revolutionize how we approach coding, mathematics, algorithm design, and why not data analysis itself.\n\nHow Does AlphaEvolve \u2018Evolve\u2019 Code?\n\nThink of it like natural selection, but for software. That is, think about Genetic Algorithms, which have existed in data science, numerical methods and computational mathematics for decades. Briefly, instead of starting from scratch every time, AlphaEvolve takes an initial piece of code \u2013 possibly a \u201cskeleton\u201d provided by a human, with specific areas marked for improvement \u2013 and then runs on it an iterative process of refinement.\n\nLet me summarize here the procedure detailed in Deepmind\u2019s white paper:\n\nIntelligent prompting: AlphaEvolve is \u201csmart\u201d enough to craft its own prompts for the underlying Gemini Llm. These prompts instruct Gemini to act like a world-class expert in a specific domain, armed with context from previous attempts, including the points that seemed to have worked correctly and those that are clear failures. This is where those massive context windows of models like Gemini (even you can run up to a million tokens at Google\u2019s AI studio) come into play.\n\nCreative mutation: The LLM then generates a diverse pool of \u201ccandidate\u201d solutions \u2013 variations and mutations of the original code, exploring different approaches to solve the given problem. This parallels very closely the inner working of regular genetic algorithms.\n\nSurvival of the fittest: Again like in genetic algorithms, but candidate solutions are automatically compiled, run, and rigorously evaluated against predefined metrics.\n\nBreeding of the top programs: The best-performing solutions are selected and become the \u201cparents\u201d for a next generation, just like in genetic algorithms. The successful traits of the parent programs are fed back into the prompting mechanism.\n\nRepeat (to evolve): This cycle \u2013 generate, test, select, learn \u2013 repeats, and with each iteration, AlphaEvolve explores the vast search space of possible programs thus gradually homing in on solutions that are better and better, while purging those that fail. The longer you let it run (what the researchers call \u201ctest-time compute\u201d), the more sophisticated and optimized the solutions can become.\n\nBuilding on Previous Attempts\n\nAlphaEvolve is the successor to earlier Google projects like AlphaCode (which tackled competitive Programming) and, more directly, of FunSearch. FunSearch was a fascinating proof of concept that showed how LLMs could discover new mathematical insights by evolving small Python functions.\n\nAlphaEvolve took that concept and \u201cinjected it with steroids\u201d. I mean this for various reasons\u2026\n\nFirst, because thanks to Gemini\u2019s huge token window, AlphaEvolve can grapple with entire codebases, hundreds of lines long, not just tiny functions as in the early tests like FunSearch. Second, because like other LLMs, Gemini has seen thousands and thousands of code in tens of programming languages; hence it has covered a wider variety of tasks (as typically different languages are used more in some domains than others) and it became a kind of polyglot programmer.\n\nNote that with smarter LLMs as engines, AlphaEvolve can itself evolve to become faster and more efficient in its search for solutions and optimal programs.\n\nAlphaEvolve\u2019s Mind-Blowing Results on Real-World Problems\n\nHere are the most interesting applications presented in the white paper:\n\nOptimizing efficiency at Google\u2019s data centers: AlphaEvolve discovered a new scheduling heuristic that squeezed out a 0.7% saving in Google\u2019s computing resources. This may look small, but Google\u2019s scale this means a substantial ecological and monetary cut!\n\nAlphaEvolve discovered a new scheduling heuristic that squeezed out a 0.7% saving in Google\u2019s computing resources. This may look small, but Google\u2019s scale this means a substantial ecological and monetary cut! Designing better AI chips: AlphaEvolve could simplify some of the complex circuits within Google\u2019s TPUs, specifically for the matrix multiplication operations that are the lifeblood of modern AI. This improves calculation speeds and again contributes to lower ecological and economical costs.\n\nAlphaEvolve could simplify some of the complex circuits within Google\u2019s TPUs, specifically for the matrix multiplication operations that are the lifeblood of modern AI. This improves calculation speeds and again contributes to lower ecological and economical costs. Faster AI training: AlphaEvolve even turned its optimization gaze inward, by accelerating a matrix multiplication library used in training the very Gemini models that power it! This means a slight but sizable reduction in AI training times and again lower ecological and economical costs!\n\nNumerical methods: In a kind of validation test, AlphaEvolve was set loose on over 50 notoriously tricky open problems in mathematics. In around 75% of them, it independently rediscovered the best-known human solutions!\n\nTowards Self-Improving AI?\n\nOne of the most profound implications of tools like AlphaEvolve is the \u201cvirtuous cycle\u201d by which AI could improve AI models themselves. Moreover, more efficient models and hardware make AlphaEvolve itself more powerful, enabling it to discover even deeper optimizations. That\u2019s a feedback loop that could dramatically accelerate AI progress, and lead who knows where. This is somehow using AI to make AI better, faster, and smarter \u2013 a genuine step on the path towards more powerful and perhaps general artificial intelligence.\n\nLeaving aside this reflection, which quickly gets close to the realm of science function, the point is that for a vast class of problems in science, engineering, and computation, AlphaEvolve could represent a paradigm shift. As a computational chemist and biologist, I myself use tools based in LLMs and reasoning AI systems to assist my work, write and debug programs, test them, analyze data more rapidly, and more. With what Deepmind has presented now, it becomes even clearer that we approach a future where AI doesn\u2019t just execute human instructions but becomes a creative partner in discovery and innovation.\n\nAlready for some months we have been moving from AI that completes our code to AI that creates it almost entirely, and tools like AlphaFold will push us to times where AI just sits to crack problems with (or for!) us, writing and evolving code to get to optimal and possibly entirely unexpected solutions. No doubt that the next few years are going to be wild.\n\nReferences and Related Reads\n\nwww.lucianoabriata.com I write about everything that lies in my broad sphere of interests: nature, science, technology, programming, etc. Subscribe to get my new stories by email. To consult about small jobs check my services page here. You can contact me here. You can tip me here."}