{"url": "https://techcrunch.com/2025/05/09/deepseek-everything-you-need-to-know-about-the-ai-chatbot-app/", "title": "DeepSeek: Everything you need to know about the AI chatbot app", "authors": ["Kyle Wiggers", "Ai Editor", "Zack Whittaker", "Rebecca Bellan", "Amanda Silberling", "Tim De Chant", "Connie Loizos", "Maxwell Zeff", "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var", "Media"], "publication_date": "2025-05-09T00:00:00", "text": "DeepSeek has gone viral.\n\nChinese AI lab DeepSeek broke into the mainstream consciousness this week after its chatbot app rose to the top of the Apple App Store charts (and Google Play, as well). DeepSeek\u2019s AI models, which were trained using compute-efficient techniques, have led Wall Street analysts \u2014 and technologists \u2014 to question whether the U.S. can maintain its lead in the AI race and whether the demand for AI chips will sustain.\n\nBut where did DeepSeek come from, and how did it rise to international fame so quickly?\n\nDeepSeek\u2019s trader origins\n\nDeepSeek is backed by High-Flyer Capital Management, a Chinese quantitative hedge fund that uses AI to inform its trading decisions.\n\nAI enthusiast Liang Wenfeng co-founded High-Flyer in 2015. Wenfeng, who reportedly began dabbling in trading while a student at Zhejiang University, launched High-Flyer Capital Management as a hedge fund in 2019 focused on developing and deploying AI algorithms.\n\nIn 2023, High-Flyer started DeepSeek as a lab dedicated to researching AI tools separate from its financial business. With High-Flyer as one of its investors, the lab spun off into its own company, also called DeepSeek.\n\nFrom day one, DeepSeek built its own data center clusters for model training. But like other AI companies in China, DeepSeek has been affected by U.S. export bans on hardware. To train one of its more recent models, the company was forced to use Nvidia H800 chips, a less-powerful version of a chip, the H100, available to U.S. companies.\n\nTechcrunch event Join us at TechCrunch Sessions: AI Secure your spot for our leading AI industry event with speakers from OpenAI, Anthropic, and Cohere. For a limited time, tickets are just $292 for an entire day of expert talks, workshops, and potent networking. Exhibit at TechCrunch Sessions: AI Secure your spot at TC Sessions: AI and show 1,200+ decision-makers what you\u2019ve built \u2014 without the big spend. Available through May 9 or while tables last. Berkeley, CA | REGISTER NOW\n\nDeepSeek\u2019s technical team is said to skew young. The company reportedly aggressively recruits doctorate AI researchers from top Chinese universities. DeepSeek also hires people without any computer science background to help its tech better understand a wide range of subjects, per The New York Times.\n\nDeepSeek\u2019s strong models\n\nDeepSeek unveiled its first set of models \u2014 DeepSeek Coder, DeepSeek LLM, and DeepSeek Chat \u2014 in November 2023. But it wasn\u2019t until last spring, when the startup released its next-gen DeepSeek-V2 family of models, that the AI industry started to take notice.\n\nDeepSeek-V2, a general-purpose text- and image-analyzing system, performed well in various AI benchmarks \u2014 and was far cheaper to run than comparable models at the time. It forced DeepSeek\u2019s domestic competition, including ByteDance and Alibaba, to cut the usage prices for some of their models, and make others completely free.\n\nDeepSeek-V3, launched in December 2024, only added to DeepSeek\u2019s notoriety.\n\nAccording to DeepSeek\u2019s internal benchmark testing, DeepSeek V3 outperforms both downloadable, openly available models like Meta\u2019s Llama and \u201cclosed\u201d models that can only be accessed through an API, like OpenAI\u2019s GPT-4o.\n\nEqually impressive is DeepSeek\u2019s R1 \u201creasoning\u201d model. Released in January, DeepSeek claims R1 performs as well as OpenAI\u2019s o1 model on key benchmarks.\n\nBeing a reasoning model, R1 effectively fact-checks itself, which helps it to avoid some of the pitfalls that normally trip up models. Reasoning models take a little longer \u2014 usually seconds to minutes longer \u2014 to arrive at solutions compared to a typical non-reasoning model. The upside is that they tend to be more reliable in domains such as physics, science, and math.\n\nThere is a downside to R1, DeepSeek V3, and DeepSeek\u2019s other models, however. Being Chinese-developed AI, they\u2019re subject to benchmarking by China\u2019s internet regulator to ensure that its responses \u201cembody core socialist values.\u201d In DeepSeek\u2019s chatbot app, for example, R1 won\u2019t answer questions about Tiananmen Square or Taiwan\u2019s autonomy.\n\nIn March, DeepSeek surpassed 16.5 million visits. \u201c[F]or March, DeepSeek is in second place, despite seeing traffic drop 25% from where it was in February, based on daily visits,\u201d David Carr, editor at Similarweb, told TechCrunch. It still pales in comparison to ChatGPT, which surged past 500 million weekly active users in March.\n\nA disruptive approach\n\nIf DeepSeek has a business model, it\u2019s not clear what that model is, exactly. The company prices its products and services well below market value \u2014 and gives others away for free. It\u2019s also not taking investor money, despite a ton of VC interest.\n\nThe way DeepSeek tells it, efficiency breakthroughs have enabled it to maintain extreme cost competitiveness. Some experts dispute the figures the company has supplied, however.\n\nWhatever the case may be, developers have taken to DeepSeek\u2019s models, which aren\u2019t open source as the phrase is commonly understood but are available under permissive licenses that allow for commercial use. According to Clem Delangue, the CEO of Hugging Face, one of the platforms hosting DeepSeek\u2019s models, developers on Hugging Face have created over 500 \u201cderivative\u201d models of R1 that have racked up 2.5 million downloads combined.\n\nDeepSeek\u2019s success against larger and more established rivals has been described as \u201cupending AI\u201d and \u201cover-hyped.\u201d The company\u2019s success was at least in part responsible for causing Nvidia\u2019s stock price to drop by 18% in January, and for eliciting a public response from OpenAI CEO Sam Altman. In March, U.S. Commerce department bureaus told staffers that DeepSeek will be banned on their government devices, according to Reuters.\n\nMicrosoft announced that DeepSeek is available on its Azure AI Foundry service, Microsoft\u2019s platform that brings together AI services for enterprises under a single banner. When asked about DeepSeek\u2019s impact on Meta\u2019s AI spending during its first-quarter earnings call, CEO Mark Zuckerberg said spending on AI infrastructure will continue to be a \u201cstrategic advantage\u201d for Meta. In March, OpenAI called DeepSeek \u201cstate-subsidized\u201d and \u201cstate-controlled,\u201d and recommends that the U.S. government consider banning models from DeepSeek.\n\nDuring Nvidia\u2019s fourth-quarter earnings call, CEO Jensen Huang emphasized DeepSeek\u2019s \u201cexcellent innovation,\u201d saying that it and other \u201creasoning\u201d models are great for Nvidia because they need so much more compute.\n\nAt the same time, some companies are banning DeepSeek, and so are entire countries and governments, including South Korea. New York state also banned DeepSeek from being used on government devices.\n\nIn May, Microsoft Vice Chairman and President Brad Smith said in a Senate hearing that Microsoft employees aren\u2019t allowed to use DeepSeek due to data security and propaganda concerns.\n\nAs for what DeepSeek\u2019s future might hold, it\u2019s not clear. Improved models are a given. But the U.S. government appears to be growing wary of what it perceives as harmful foreign influence. In March, The Wall Street Journal reported that the U.S. will likely ban DeepSeek on government devices.\n\nThis story was originally published January 28, 2025, and will be updated regularly."}